{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "chromedriver = \"/Users/stevenwang/Projects/funded-and-hiring-site/newsletter_scraper/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "import pyperclip\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pytest_timeout"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "source": [
    "def get_axios_deals(days_back):\n",
    "    url = 'https://www.axios.com/authors/danprimack/newsletters'\n",
    "#     url = 'https://www.axios.com/authors/kiakokalitcheva/newsletters'\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    companies = []\n",
    "    deal_text = []\n",
    "    publish_date = []\n",
    "    stories = []\n",
    "    company_urls = []\n",
    "\n",
    "    #grab last 5 edition urls, check to make sure they are all within the last week\n",
    "    news = soup.find_all('amp-layout')[:days_back]\n",
    "    for letter in news:\n",
    "        #check if date of newsletter is in last week, if not then pass\n",
    "        date = letter.find_next('span', attrs = {'data-testid':'time-rubric'}).text\n",
    "        if 'hours' in date or 'mins' in date or 'hour' in date:\n",
    "            date = pd.to_datetime(datetime.now().date())\n",
    "        else:\n",
    "            date = pd.to_datetime(date)\n",
    "        if date < pd.to_datetime(datetime.now().date()) - timedelta(days = days_back):\n",
    "            break\n",
    "\n",
    "        #get company names and deal texts mentioned in each news letter\n",
    "        letter_url = letter.find_next('a', href = True, class_ = 'title-link gtm-content-click')['href']\n",
    "        letter_companies, letter_deal_text, letter_story_urls = get_axios_deal_text(letter_url)\n",
    "\n",
    "        #append these to the running list\n",
    "        companies += letter_companies\n",
    "        deal_text += letter_deal_text\n",
    "        publish_date += [date for i in range(len(letter_companies))]\n",
    "        stories += letter_story_urls\n",
    "    \n",
    "    #grab company site urls\n",
    "    for company, story in zip(companies, stories):\n",
    "        company = company.lower().strip()\n",
    "        if company in story: #if company name is in story url, then use story url as company url\n",
    "            company_urls.append(story)\n",
    "        else:#grab company url from story\n",
    "            try:\n",
    "                company_site = get_company_url_from_axios_story(company, story)\n",
    "                company_urls.append(company_site)\n",
    "            except: #if company url can't be found in the story, use story url instead\n",
    "                company_urls.append(story)\n",
    "                \n",
    "    dataframe = pd.DataFrame({'company':companies, 'deal_text':deal_text, 'publish_date':publish_date, 'story_url':stories, 'company_url':company_urls})\n",
    "    return dataframe\n",
    "\n",
    "def get_axios_deal_text(link):\n",
    "    prorata_soup =  BeautifulSoup(requests.get(link).content, \"html.parser\")\n",
    "    vc_deals = prorata_soup.find('div', text = re.compile('Venture Capital Deals')).find_next('div', class_ = re.compile('.*story-text.*')).find_all('p')\n",
    "    \n",
    "    companies = []\n",
    "    deal_text = []\n",
    "    stories = []\n",
    "\n",
    "    for deal in vc_deals:\n",
    "        company_name = deal.find_next('strong').text.strip()\n",
    "        if company_name == 'â€¢':\n",
    "            company_name = deal.find_next('strong').find_next('strong').text.strip()\n",
    "\n",
    "        text = deal.text\n",
    "        text = re.sub(r'[^-a-zA-z0-9.,!? ]', '', text).strip()\n",
    "        story_url = deal.find_next('a', class_ = 'gtm-content-click')['href']\n",
    "\n",
    "        companies.append(company_name)\n",
    "        deal_text.append(text)\n",
    "        stories.append(story_url)\n",
    "    return(companies, deal_text, stories)\n",
    "\n",
    "def get_company_url_from_axios_story(company_name, story_url):\n",
    "    page = requests.get(story_url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    article_url = soup.find('meta', attrs={'property': 'twitter:url'})['content']\n",
    "    headers = {'user-agent':'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Mobile Safari/537.36'}\n",
    "    article_page = requests.get(article_url, headers = headers)\n",
    "    article_soup = BeautifulSoup(article_page.content, \"html.parser\") \n",
    "\n",
    "    potential_company_urls = []\n",
    "    #get hyperlink url that contains company name\n",
    "    company_name_no_space = company_name.replace(' ', '')\n",
    "    company_name_no_space_url = article_soup.find_all('a', href = re.compile('.*{company_name_no_space}'.format(company_name_no_space = company_name_no_space), re.IGNORECASE))\n",
    "    if len(company_name_no_space_url) > 0:\n",
    "        potential_company_urls.append(company_name_no_space_url[0]['href'])\n",
    "    \n",
    "    company_name_hyphen = company_name.replace(' ', '-')\n",
    "    company_name_hyphen_url = article_soup.find_all('a', href = re.compile('.*{company_name_hyphen}'.format(company_name_hyphen = company_name_hyphen), re.IGNORECASE))\n",
    "    if len(company_name_hyphen_url) > 0:\n",
    "        potential_company_urls.append(company_name_hyphen_url[0]['href'])\n",
    "        \n",
    "    company_name_url = article_soup.find_all('a', text = re.compile('.*{company_name}.*'.format(company_name = company_name), re.IGNORECASE))\n",
    "    if len(company_name_url)  > 0:\n",
    "        potential_company_urls.append(company_name_url[0]['href'])\n",
    "    \n",
    "    #return the shortest url\n",
    "    return min(potential_company_urls, key=len)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "source": [
    "def clean_axios_deals(deals):\n",
    "    #get deal size\n",
    "    deals['deal_size'] = deals.deal_text.str.extract(r'((?:\\d+[.]\\d+|\\d+|\\d+[.]\\d+ |\\d+ )(?:million|billion|Million|Billion))')\n",
    "    \n",
    "    #only keep deals that have dollar amounts\n",
    "    deals = deals[deals.deal_size.notnull()]\n",
    "    \n",
    "    #convert to an actual number\n",
    "    deals.loc[deals.deal_size.str.contains('million|m$', case = False), 'deal_size'] = deals.deal_size.str.split(' ').str[0].astype(float) * 1e6\n",
    "    deals['deal_size'] = deals.deal_size.astype(str)\n",
    "    deals.loc[deals.deal_size.str.contains('billion|b$', case = False), 'deal_size'] = deals.deal_size.str.split(' ').str[0].astype(float) * 1e9\n",
    "    \n",
    "    #remove urls from deal text\n",
    "    deals['deal_text'] = deals.deal_text.str.split('www[.]|http').str[0].str.strip()\n",
    "    return deals"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "source": [
    "axios = get_axios_deals(7)\n",
    "\n",
    "axios\n",
    "\n",
    "axios = clean_axios_deals(axios)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "source": [
    "def get_finsme_deals(days_back):\n",
    "    page = 1\n",
    "    past_date = False\n",
    "\n",
    "    companies = []\n",
    "    deal_text = []\n",
    "    publish_date = []\n",
    "    stories = []\n",
    "    company_urls = []\n",
    "    while page <= days_back and past_date == False:\n",
    "        url = 'http://www.finsmes.com/category/usa/page/' + str(page)\n",
    "        soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "        deals = soup.find_all('article')\n",
    "        for d in deals:\n",
    "            #check if date of article is > 7 days ago, if true stop\n",
    "            date = pd.to_datetime(d.find_next('time', attrs = {'itemprop':'datePublished'}).text)\n",
    "            if date < pd.to_datetime(datetime.now().date()) - timedelta(days = days_back):\n",
    "                past_date = True\n",
    "                break\n",
    "            else:\n",
    "                text = d.find_next('p').text.strip().replace('\\n', '')\n",
    "                company_name = text.split(',')[0].strip()\n",
    "                story_url = d.find('a', attrs = {'itemprop':'url'})['href']\n",
    "\n",
    "                companies.append(company_name)\n",
    "                deal_text.append(text)\n",
    "                publish_date.append(date)\n",
    "                stories.append(story_url)\n",
    "\n",
    "        page += 1\n",
    "    #grab company site urls\n",
    "    for company, story in zip(companies, stories):\n",
    "        company_site = get_company_url_from_finsmes_story(company, story)\n",
    "        company_urls.append(company_site)\n",
    "\n",
    "    dataframe = pd.DataFrame({'company':companies, 'deal_text':deal_text, \n",
    "                              'publish_date':publish_date, 'story_url':stories, 'company_url':company_urls})\n",
    "    return dataframe\n",
    "\n",
    "def clean_finsme_deals(deals):\n",
    "    #remove acquisitions\n",
    "    deals = deals[(~deals.company.str.contains('acquire|acquisition')) & (~deals.deal_text.str.contains('acquire|acquisition'))]\n",
    "    \n",
    "    #remove deals where company name is hard to parse\n",
    "    deals = deals[deals.company.str.split(' ').str.len() <= 5]\n",
    "    \n",
    "    #get deal size\n",
    "    deals['deal_size'] = deals.deal_text.str.extract(r'((?:\\d+[.]\\d+|\\d+)(?:million|billion|m|b|M))')\n",
    "    deals = deals[deals.deal_size.notnull()]\n",
    "    \n",
    "    deals.loc[deals.deal_size.str.contains('million|m$', case = False), 'deal_size'] = deals.deal_size.str.split('m|M').str[0].astype(float) * 1e6\n",
    "    deals['deal_size'] = deals.deal_size.astype(str)\n",
    "    deals.loc[deals.deal_size.str.contains('billion|b$', case = False), 'deal_size'] = deals.deal_size.str.split('b|B').str[0].astype(float) * 1e9\n",
    "    \n",
    "    \n",
    "    return deals\n",
    "\n",
    "def get_company_url_from_finsmes_story(company_name, story_url):\n",
    "    headers = {'user-agent':'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Mobile Safari/537.36'}\n",
    "    article_page = requests.get(story_url, headers = headers)\n",
    "    article_soup = BeautifulSoup(article_page.content, \"html.parser\") \n",
    "    article_content = article_soup.find('div', class_ = 'entry-content')\n",
    "    try:\n",
    "        company_url = article_content.find_all('a', text = re.compile('.*{company_name}.*'.format(company_name = company_name), re.IGNORECASE))[0]['href']\n",
    "    except:\n",
    "        company_url = story_url\n",
    "    return company_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "source": [
    "finsme = get_finsme_deals(7)\n",
    "finsme = clean_finsme_deals(finsme)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "source": [
    "vc_deals = pd.concat([axios, finsme], axis = 0)\n",
    "#replace all .coms in company names\n",
    "vc_deals['company'] = vc_deals.company.str.replace(\"[.].*|â€¢|â€™|,|[+]|[()]|[#]\", '').str.strip()\n",
    "\n",
    "#remove unicode spaces\n",
    "vc_deals['company'] = vc_deals.company.str.replace(u'\\xa0|\\u200d', ' ').str.strip()\n",
    "vc_deals['company'] = vc_deals.company.str.replace('&', 'and').str.strip()\n",
    "vc_deals['company'] = vc_deals.company.str.replace('[/]', '').str.strip()\n",
    "\n",
    "vc_deals.loc[vc_deals.company == 'ðŸ§€', 'company'] = 'Nobell Foods'\n",
    "vc_deals['company'] = vc_deals.company.str.replace('â˜•', '')\n",
    "vc_deals['company'] = vc_deals.company.str.replace('ðŸš™', '')\n",
    "vc_deals['company'] = vc_deals.company.str.replace('ðŸš‘', '')\n",
    "vc_deals['company'] = vc_deals.company.str.replace('ðŸ¶', '')\n",
    "vc_deals['company'] = vc_deals.company.str.replace('â›½ï¸|ðŸ‰', '')\n",
    "\n",
    "\n",
    "vc_deals['company'] = vc_deals.company.str.strip()\n",
    "vc_deals = vc_deals[vc_deals.company != '']\n",
    "vc_deals = vc_deals[vc_deals.company != 'Vestiaire Collective']\n",
    "\n",
    "\n",
    "\n",
    "vc_deals = vc_deals[~vc_deals.company.str.upper().duplicated()]\n",
    "vc_deals['deal_size'] = vc_deals.deal_size.astype(float)\n",
    "vc_deals = vc_deals.sort_values('deal_size', ascending = False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "source": [
    "len(vc_deals)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "metadata": {},
     "execution_count": 261
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "source": [
    "vc_deals"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>deal_text</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>story_url</th>\n",
       "      <th>company_url</th>\n",
       "      <th>deal_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Devoted Health</td>\n",
       "      <td>Devoted Health, a Waltham, Mass.-based health ...</td>\n",
       "      <td>2021-09-24</td>\n",
       "      <td>http://axios.link/9ly4</td>\n",
       "      <td>http://axios.link/9ly4</td>\n",
       "      <td>1.200000e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>PandaDoc</td>\n",
       "      <td>PandaDoc, an SF-based e-document company, rais...</td>\n",
       "      <td>2021-09-23</td>\n",
       "      <td>http://axios.link/qdcX</td>\n",
       "      <td>https://www.pandadoc.com/</td>\n",
       "      <td>1.000000e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Ninja</td>\n",
       "      <td>Ninja Van, a Singapore-based package delivery ...</td>\n",
       "      <td>2021-09-27</td>\n",
       "      <td>http://axios.link/1N5N</td>\n",
       "      <td>http://axios.link/1N5N</td>\n",
       "      <td>5.780000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meesho</td>\n",
       "      <td>Meesho, an Indian social e-commerce company, r...</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>http://axios.link/9w8i</td>\n",
       "      <td>http://axios.link/9w8i</td>\n",
       "      <td>5.700000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Atome</td>\n",
       "      <td>Atome, a Singapore-based BNPL startup, raised ...</td>\n",
       "      <td>2021-09-23</td>\n",
       "      <td>http://axios.link/qxF6</td>\n",
       "      <td>http://axios.link/qxF6</td>\n",
       "      <td>4.000000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Sanome</td>\n",
       "      <td>Sanome, a British biomarkers startup, raised 2...</td>\n",
       "      <td>2021-09-28</td>\n",
       "      <td>http://www.sanome.com/</td>\n",
       "      <td>http://www.sanome.com/</td>\n",
       "      <td>2.000000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Noops</td>\n",
       "      <td>Noops, a plant-based pudding startup based in ...</td>\n",
       "      <td>2021-09-27</td>\n",
       "      <td>http://axios.link/GxAg</td>\n",
       "      <td>https://eatnoops.com/</td>\n",
       "      <td>2.000000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Climatiq</td>\n",
       "      <td>Climatiq, a Berlin-based carbon footprint inte...</td>\n",
       "      <td>2021-09-23</td>\n",
       "      <td>http://www.climatiq.io/</td>\n",
       "      <td>http://www.climatiq.io/</td>\n",
       "      <td>2.000000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Gruvi</td>\n",
       "      <td>Gruvi, a Denver CO-based line of non-alcoholic...</td>\n",
       "      <td>2021-09-29</td>\n",
       "      <td>https://www.finsmes.com/2021/09/gruvi-raises-2...</td>\n",
       "      <td>https://www.getgruvi.com/</td>\n",
       "      <td>2.000000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>General Lattice</td>\n",
       "      <td>General Lattice, a Chicago, IL-based advanced ...</td>\n",
       "      <td>2021-09-24</td>\n",
       "      <td>https://www.finsmes.com/2021/09/general-lattic...</td>\n",
       "      <td>http://www.generallattice.com</td>\n",
       "      <td>1.000000e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             company                                          deal_text  \\\n",
       "99    Devoted Health  Devoted Health, a Waltham, Mass.-based health ...   \n",
       "138         PandaDoc  PandaDoc, an SF-based e-document company, rais...   \n",
       "83             Ninja  Ninja Van, a Singapore-based package delivery ...   \n",
       "0             Meesho  Meesho, an Indian social e-commerce company, r...   \n",
       "112            Atome  Atome, a Singapore-based BNPL startup, raised ...   \n",
       "..               ...                                                ...   \n",
       "82            Sanome  Sanome, a British biomarkers startup, raised 2...   \n",
       "98             Noops  Noops, a plant-based pudding startup based in ...   \n",
       "137         Climatiq  Climatiq, a Berlin-based carbon footprint inte...   \n",
       "20             Gruvi  Gruvi, a Denver CO-based line of non-alcoholic...   \n",
       "75   General Lattice  General Lattice, a Chicago, IL-based advanced ...   \n",
       "\n",
       "    publish_date                                          story_url  \\\n",
       "99    2021-09-24                             http://axios.link/9ly4   \n",
       "138   2021-09-23                             http://axios.link/qdcX   \n",
       "83    2021-09-27                             http://axios.link/1N5N   \n",
       "0     2021-09-30                             http://axios.link/9w8i   \n",
       "112   2021-09-23                             http://axios.link/qxF6   \n",
       "..           ...                                                ...   \n",
       "82    2021-09-28                             http://www.sanome.com/   \n",
       "98    2021-09-27                             http://axios.link/GxAg   \n",
       "137   2021-09-23                            http://www.climatiq.io/   \n",
       "20    2021-09-29  https://www.finsmes.com/2021/09/gruvi-raises-2...   \n",
       "75    2021-09-24  https://www.finsmes.com/2021/09/general-lattic...   \n",
       "\n",
       "                       company_url     deal_size  \n",
       "99          http://axios.link/9ly4  1.200000e+09  \n",
       "138      https://www.pandadoc.com/  1.000000e+09  \n",
       "83          http://axios.link/1N5N  5.780000e+08  \n",
       "0           http://axios.link/9w8i  5.700000e+08  \n",
       "112         http://axios.link/qxF6  4.000000e+08  \n",
       "..                             ...           ...  \n",
       "82          http://www.sanome.com/  2.000000e+06  \n",
       "98           https://eatnoops.com/  2.000000e+06  \n",
       "137        http://www.climatiq.io/  2.000000e+06  \n",
       "20       https://www.getgruvi.com/  2.000000e+06  \n",
       "75   http://www.generallattice.com  1.000000e+06  \n",
       "\n",
       "[190 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 262
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "source": [
    "# def get_lever_jobs(companies):\n",
    "#     company_ls = []\n",
    "#     job_ls = []\n",
    "#     location_ls = []\n",
    "#     listing_url = []\n",
    "#     no_lever = []\n",
    "\n",
    "#     for company in companies:\n",
    "#         company_cleaned = company.replace(' ', '').lower()\n",
    "#         url = 'https://jobs.lever.co/' + company_cleaned\n",
    "#         soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "#         jobs = soup.find_all('div', class_ = 'posting')\n",
    "\n",
    "#         #if no jobs found, add to no lever list\n",
    "#         if len(jobs) == 0:\n",
    "#             no_lever.append(company)\n",
    "#         else:\n",
    "#             for job in jobs:\n",
    "#                 job_name = job.find('h5', attrs = {'data-qa':'posting-name'}).text.strip()\n",
    "\n",
    "#                 job_location = job.find('span', attrs = {'class':'sort-by-location posting-category small-category-label'})\n",
    "#                 if job_location == None: #not all jobs have location, set to none if missing\n",
    "#                     job_location = None\n",
    "#                 else:\n",
    "#                     job_location = job_location.text.strip()\n",
    "\n",
    "#                 company_ls.append(company)\n",
    "#                 job_ls.append(job_name)\n",
    "#                 location_ls.append(job_location)\n",
    "#                 listing_url.append(url)\n",
    "#     df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})\n",
    "    \n",
    "#     print(len(companies) - len(no_lever), 'out of ', len(companies), 'companies found on Lever')\n",
    "#     return df, no_lever\n",
    "    "
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "source": [
    "# lever_jobs, not_on_lever = get_lever_jobs(vc_deals.company)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "source": [
    "# import signal\n",
    "\n",
    "# class Timeout:\n",
    "#     def __init__(self, seconds=1, error_message='Timeout'):\n",
    "#         self.seconds = seconds\n",
    "#         self.error_message = error_message\n",
    "#     def handle_timeout(self, signum, frame):\n",
    "#         raise TimeoutError(self.error_message)\n",
    "#     def __enter__(self):\n",
    "#         signal.signal(signal.SIGALRM, self.handle_timeout)\n",
    "#         signal.alarm(self.seconds)\n",
    "#     def __exit__(self, type, value, traceback):\n",
    "#         signal.alarm(0)\n",
    "\n",
    "# def test_it_doesnt_succeed():\n",
    "#     try:\n",
    "#         with Timeout(seconds=6):\n",
    "#             do_the_thing()\n",
    "#     except TimeoutError:\n",
    "#         pass\n",
    "#     else:\n",
    "#         raise AssertionError('Expected the thing to timeout!')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "source": [
    "# company_ls = []\n",
    "# job_ls = []\n",
    "# location_ls = []\n",
    "# listing_url = []\n",
    "# no_greenhouse = []\n",
    "\n",
    "# for company in companies:\n",
    "#     company_cleaned = company.replace(' ', '')\n",
    "#     url = 'https://boards.greenhouse.io/' + company_cleaned\n",
    "#     print(url)\n",
    "#     try:\n",
    "# #         test_it_doesnt_succeed(requests.get(url).content, 'html.parser')\n",
    "#         soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "#     except:\n",
    "#         no_greenhouse.append(company)\n",
    "#         continue\n",
    "#     jobs = soup.find_all('div', class_ = 'opening')\n",
    "\n",
    "#     #if no jobs found, add to no lever list\n",
    "#     if len(jobs) == 0:\n",
    "#         no_greenhouse.append(company)\n",
    "#     else:\n",
    "#         for job in jobs:\n",
    "#             job_name = job.find('a', attrs = {'data-mapped':\"true\"}).text.strip()\n",
    "#             job_location = job.find('span', class_ = 'location').text.strip()\n",
    "\n",
    "#             company_ls.append(company)\n",
    "#             job_ls.append(job_name)\n",
    "#             location_ls.append(job_location)\n",
    "#             listing_url.append(url)\n",
    "# df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "source": [
    "# def get_greenhouse_jobs(companies):\n",
    "#     company_ls = []\n",
    "#     job_ls = []\n",
    "#     location_ls = []\n",
    "#     listing_url = []\n",
    "#     no_greenhouse = []\n",
    "\n",
    "#     for company in companies:\n",
    "#         company_cleaned = company.replace(' ', '')\n",
    "#         url = 'https://boards.greenhouse.io/' + company_cleaned\n",
    "#         try:\n",
    "#             soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "#         except:\n",
    "#             no_greenhouse.append(company)\n",
    "#             continue\n",
    "#         jobs = soup.find_all('div', class_ = 'opening')\n",
    "\n",
    "#         #if no jobs found, add to no lever list\n",
    "#         if len(jobs) == 0:\n",
    "#             no_greenhouse.append(company)\n",
    "#         else:\n",
    "#             for job in jobs:\n",
    "#                 job_name = job.find('a', attrs = {'data-mapped':\"true\"}).text.strip()\n",
    "#                 job_location = job.find('span', class_ = 'location').text.strip()\n",
    "\n",
    "#                 company_ls.append(company)\n",
    "#                 job_ls.append(job_name)\n",
    "#                 location_ls.append(job_location)\n",
    "#                 listing_url.append(url)\n",
    "#     df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})\n",
    "#     print(len(companies) - len(no_greenhouse), 'out of ', len(companies), 'companies found on greenhouse')\n",
    "#     return(df, no_greenhouse)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "source": [
    "# greenhouse_jobs, not_on_greenhouse = get_greenhouse_jobs(not_on_lever)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "source": [
    "# def get_breezy_jobs(companies):\n",
    "#     headers = {'user-agent':'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Mobile Safari/537.36'}\n",
    "    \n",
    "#     company_ls = []\n",
    "#     job_ls = []\n",
    "#     location_ls = []\n",
    "#     listing_url = []\n",
    "#     no_breezy = []\n",
    "    \n",
    "#     for company in companies:\n",
    "#         company_cleaned = company.replace(' ', '').lower() \n",
    "#         url = 'https://' + company_cleaned + '.breezy.hr'\n",
    "#         soup = BeautifulSoup(requests.get(url, headers = headers).content, 'html.parser')\n",
    "        \n",
    "#         jobs = soup.find_all('li', attrs = {'class':'position transition'})\n",
    "        \n",
    "#         #if no jobs found, add to no breezy list\n",
    "#         if len(jobs) == 0:\n",
    "#             no_breezy.append(company)\n",
    "#         else:\n",
    "#             for job in jobs:\n",
    "#                 job_name = job.find_next('h2').text.strip()\n",
    "#                 job_location = job.find_next('li', attrs = {'class':'location'}).text.strip()\n",
    "                \n",
    "#                 company_ls.append(company)\n",
    "#                 job_ls.append(job_name)\n",
    "#                 location_ls.append(job_location)\n",
    "#                 listing_url.append(url)\n",
    "                \n",
    "#     df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})\n",
    "#     print(len(companies) - len(no_breezy), 'out of ', len(companies), 'companies found on breezy')\n",
    "#     return df, no_breezy\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "source": [
    "# breezy_jobs, not_on_breezy = get_breezy_jobs(not_on_greenhouse)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "source": [
    "# def get_google_jobs(companies):\n",
    "#     company_ls = []\n",
    "#     job_ls = []\n",
    "#     location_ls = []\n",
    "#     listing_url = []\n",
    "#     no_google = []\n",
    "\n",
    "#     for company in companies:\n",
    "#         company_cleaned = company.replace(' ', '').lower()\n",
    "#         url = 'https://hire.withgoogle.com/public/jobs/' + company_cleaned\n",
    "#         soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "#         jobs = soup.find_all('li', class_ = 'bb-public-jobs-list__job-item ptor-jobs-list__item')\n",
    "\n",
    "#         #if no jobs found, add to no lever list\n",
    "#         if len(jobs) == 0:\n",
    "#             no_google.append(company)\n",
    "#         else:\n",
    "#             for job in jobs:\n",
    "#                 job_name = job.find('span', class_ = 'bb-public-jobs-list__job-item-title ptor-jobs-list__item-job-title').text.strip()\n",
    "#                 job_location = job.find('span', class_ = 'bb-public-jobs-list__job-item-location ptor-jobs-list__item-location').text.strip()\n",
    "\n",
    "#                 company_ls.append(company)\n",
    "#                 job_ls.append(job_name)\n",
    "#                 location_ls.append(job_location)\n",
    "#                 listing_url.append(url)\n",
    "#     df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})\n",
    "#     print(len(companies) - len(no_google), 'out of ', len(companies), 'companies found on hirewithgoogle')\n",
    "#     return(df, no_google)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "source": [
    "# google_jobs, not_on_google = get_google_jobs(not_on_greenhouse)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "source": [
    "# def get_angel_jobs(companies):\n",
    "# #     driver = webdriver.Chrome(chromedriver)\n",
    "    \n",
    "#     company_ls = []\n",
    "#     job_ls = []\n",
    "#     location_ls = []\n",
    "#     listing_url = []\n",
    "#     no_angel = []\n",
    "\n",
    "#     for company in companies:\n",
    "#         driver = webdriver.Chrome(chromedriver)\n",
    "#         company_cleaned = company.replace(' ', '').lower()\n",
    "#         url = 'https://angel.co/company/' + company_cleaned + '/jobs'\n",
    "#         driver.get(url)\n",
    "        \n",
    "#         soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "#         jobs = soup.find_all('div', class_ = 'styles_component__1_YxE styles_expanded__31zII')\n",
    "\n",
    "#         #if no jobs found, add to no lever list\n",
    "#         if len(jobs) == 0:\n",
    "#             no_angel.append(company)\n",
    "#         else:\n",
    "#             for job in jobs:\n",
    "#                 job_name = job.find('div', class_ = 'breakpoint__desktop-up').text.strip()\n",
    "#                 job_location = job.find('div', class_ = 'styles_component__26gqE styles_truncate__dUufp styles_location__ACesY').text.strip()\n",
    "\n",
    "#                 company_ls.append(company)\n",
    "#                 job_ls.append(job_name)\n",
    "#                 location_ls.append(job_location)\n",
    "#                 listing_url.append(url)\n",
    "\n",
    "#         driver.quit()\n",
    "#     df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})\n",
    "#     print(len(companies) - len(no_angel), 'out of ', len(companies), 'companies found on angel.co')\n",
    "# #     driver.quit()\n",
    "#     return(df, no_angel)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "source": [
    "# angel_jobs, not_on_angel = get_angel_jobs(not_on_greenhouse)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "source": [
    "# def get_smartrecruiter_jobs(companies):\n",
    "#     company_ls = []\n",
    "#     job_ls = []\n",
    "#     location_ls = []\n",
    "#     listing_url = []\n",
    "#     no_smartrecruiter = []\n",
    "\n",
    "#     for company in companies:\n",
    "#         company_cleaned = company.replace(' ', '')\n",
    "#         url = 'https://careers.smartrecruiters.com/' + company_cleaned\n",
    "#         soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "\n",
    "#         locations = soup.find_all('section', class_ = 'openings-section opening opening--grouped js-group')\n",
    "        \n",
    "#         if len(locations) == 0:\n",
    "#             no_smartrecruiter.append(company)\n",
    "#         else:\n",
    "#             for loc in locations:\n",
    "#                 job_location = loc.find('h3', class_ = 'opening-title title display--inline-block text--default').text.strip()\n",
    "#                 jobs = loc.find_all('h4', class_ = 'details-title job-title link--block-target')\n",
    "#                 for job in jobs:\n",
    "#                     company_ls.append(company)\n",
    "#                     job_name = job.text.strip()\n",
    "#                     job_ls.append(job_name)\n",
    "#                     location_ls.append(job_location)\n",
    "#                     listing_url.append(url)\n",
    "                    \n",
    "#     df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})\n",
    "#     print(len(companies) - len(no_smartrecruiter), 'out of ', len(companies), 'companies found on smartrecruiter')\n",
    "#     return(df, no_smartrecruiter)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "source": [
    "# sr_jobs, not_on_sr = get_smartrecruiter_jobs(not_on_breezy)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "source": [
    "# def get_workable_jobs(companies):\n",
    "#     company_ls = []\n",
    "#     job_ls = []\n",
    "#     location_ls = []\n",
    "#     listing_url = []\n",
    "#     no_workable = []\n",
    "    \n",
    "#     for company in companies:\n",
    "#         driver = webdriver.Chrome(chromedriver)\n",
    "#         company_cleaned = company.replace(' ', '').lower()\n",
    "#         url = 'https://apply.workable.com/' + company_cleaned\n",
    "\n",
    "#         driver.get(url)\n",
    "#         try:\n",
    "#         #wait until job list shows up\n",
    "#             WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, \"//li[@data-ui='job-opening']\")))\n",
    "#         except:\n",
    "#             no_workable.append(company)\n",
    "#             driver.quit()\n",
    "#             continue\n",
    "#         soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "#         #click load more jobs\n",
    "#         load_more_button = soup.find_all(attrs = {'data-ui':'load-more-button'})\n",
    "#         while len(load_more_button) > 0:\n",
    "#             btn = driver.find_element_by_xpath(\"//button[@data-ui='load-more-button']\")\n",
    "#             btn.click()\n",
    "#         #     try:\n",
    "#         #         WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, \"//button[@data-ui='load-more-button']\")))\n",
    "#         #     except:\n",
    "#         #         pass\n",
    "#             time.sleep(2)\n",
    "#             soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "#             load_more_button = soup.find_all(attrs = {'data-ui':'load-more-button'})\n",
    "        \n",
    "#         jobs_list = soup.find_all('li', attrs = {'data-ui':'job-opening'})\n",
    "\n",
    "#         if len(jobs_list) == 0:\n",
    "#             no_workable.append(company)\n",
    "#         else:\n",
    "#             for job in jobs_list:\n",
    "#                 job_name = job.find_next(attrs = {'data-id':'job-item'}).text\n",
    "#                 job_url = 'https://apply.workable.com' + job.find_next('a')['href']\n",
    "#                 try:\n",
    "#                     job_location = job.find_next(attrs = {'data-ui':'job-location'}).text\n",
    "#                 except:\n",
    "#                     job_location = None\n",
    "                \n",
    "#                 #check if remote job\n",
    "#                 if len(job.find_all(attrs = {'data-ui':'job-remote'})) > 0:\n",
    "#                     job_location = 'Remote'\n",
    "\n",
    "#                 company_ls.append(company)\n",
    "#                 job_ls.append(job_name)\n",
    "#                 location_ls.append(job_location)\n",
    "#                 listing_url.append(url)\n",
    "            \n",
    "#         driver.quit()\n",
    "#     df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})\n",
    "#     print(len(companies) - len(no_workable), 'out of ', len(companies), 'companies found on workable')\n",
    "#     return(df, no_workable)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "source": [
    "# workable_jobs, not_on_workable = get_workable_jobs(not_on_sr)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "source": [
    "# driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "# url = 'https://apply.workable.com/zenoti'\n",
    "# driver.get(url)\n",
    "\n",
    "# #wait until job list shows up\n",
    "# WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//li[@data-ui='job-opening']\")))\n",
    "\n",
    "# soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# #click load more jobs\n",
    "# load_more_button = soup.find_all(attrs = {'data-ui':'load-more-button'})\n",
    "# while len(load_more_button) > 0:\n",
    "#     btn = driver.find_element_by_xpath(\"//button[@data-ui='load-more-button']\")\n",
    "#     btn.click()\n",
    "# #     try:\n",
    "# #         WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, \"//button[@data-ui='load-more-button']\")))\n",
    "# #     except:\n",
    "# #         pass\n",
    "#     time.sleep(2)\n",
    "#     soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "#     load_more_button = soup.find_all(attrs = {'data-ui':'load-more-button'})\n",
    "\n",
    "# jobs_list = soup.find_all('li', attrs = {'data-ui':'job-opening'})\n",
    "\n",
    "# for job in jobs_list:\n",
    "#     job_name = job.find_next(attrs = {'data-id':'job-item'}).text\n",
    "#     job_url = 'https://apply.workable.com' + job.find_next('a')['href']\n",
    "    \n",
    "#     job_location = job.find_next(attrs = {'data-ui':'job-location'}).text\n",
    "#     #check if remote job\n",
    "#     if len(job.find_all(attrs = {'data-ui':'job-remote'})) > 0:\n",
    "#         job_location = 'Remote'\n",
    "#     print(job_name, job_url, job_location)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "source": [
    "# def get_indeed_jobs(companies):\n",
    "#     company_ls = []\n",
    "#     job_ls = []\n",
    "#     location_ls = []\n",
    "#     listing_url = []\n",
    "#     no_indeed = []\n",
    "#     headers = {'user-agent':'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Mobile Safari/537.36'}\n",
    "\n",
    "\n",
    "#     for company in companies:\n",
    "#         company_cleaned = company.replace(' ', '-')\n",
    "#         url = 'https://www.indeed.com/cmp/' + company_cleaned + '/jobs?q=&l=#cmp-skip-header-desktop'\n",
    "#         soup = BeautifulSoup(requests.get(url, headers = headers).content, 'html.parser')\n",
    "#         jobs = soup.find_all('li', class_ = 'cmp-JobListItem')\n",
    "\n",
    "#         #if no jobs found, add to no lever list\n",
    "#         if len(jobs) == 0:\n",
    "#             no_indeed.append(company)\n",
    "#         else:\n",
    "#             for job in jobs:\n",
    "#                 job_name = job.find('div', class_ = 'cmp-JobListItem-title').text.strip()\n",
    "#                 job_location = job.find('div', class_ = 'cmp-JobListItem-subtitle').text.strip()\n",
    "\n",
    "#                 company_ls.append(company)\n",
    "#                 job_ls.append(job_name)\n",
    "#                 location_ls.append(job_location)\n",
    "#                 listing_url.append(url)\n",
    "#     df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})\n",
    "#     print(len(companies) - len(no_indeed), 'out of ', len(companies), 'companies found on indeed')\n",
    "#     return(df, no_indeed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "source": [
    "# indeed_jobs, not_on_indeed = get_indeed_jobs(not_on_workable)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "source": [
    "#concat jobs\n",
    "# jobs = pd.concat([lever_jobs, greenhouse_jobs, indeed_jobs, sr_jobs, workable_jobs], axis = 0)\n",
    "# jobs = lever_jobs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "source": [
    "# len(jobs.company.unique())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "source": [
    "# jobs.job.unique().tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "source": [
    "#remote jobs\n",
    "# jobs['Remote'] = 0\n",
    "# jobs.loc[(jobs.location.str.contains('Remote', case = False, na = False)) | \n",
    "#          (jobs.job.str.contains('Remote', case = False, na = False)), 'Remote'] = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "source": [
    "#job categories\n",
    "# jobs['Engineering/Technology'] = 0\n",
    "# eng_str = 'engineer|developer|quality assurance|Technical Lead|tech lead|devops|database admin|(machine|deep) learning|scientist|\\Wqa\\W|infrastructure|\\WML\\W|tech.*lead|architect|chemist|research|geneticist|computational|informatics|scientist|\\WIT\\W|programmer|\\WML\\W|biolog|pharma|oncology|chemical|dev op|computational|cloud|Bioinformatician'\n",
    "# jobs.loc[jobs.job.str.contains(eng_str, case = False), 'Engineering/Technology'] = 1\n",
    "\n",
    "# jobs['Data'] = 0\n",
    "# data_str = 'database admin|(machine|deep) learning|scientist|data|analytics|\\WML\\W|business intelligence|BI Analyst|\\WAI\\W|product analyst|statistician|\\WBI\\W|DBA'\n",
    "# jobs.loc[jobs.job.str.contains(data_str, case = False), 'Data'] = 1\n",
    "\n",
    "# #group data into engineering/technology\n",
    "# jobs.loc[jobs.Data == 1, 'Engineering/Technology'] = 1\n",
    "\n",
    "# jobs['Design'] = 0\n",
    "# design_str = 'design|user experience|\\Wux\\W|\\Wui\\W|creative director|\\Wart\\W|graphic'\n",
    "# jobs.loc[jobs.job.str.contains(design_str, case = False), 'Design'] = 1\n",
    "\n",
    "# jobs['Product'] = 0\n",
    "# prod_str = 'product'\n",
    "# jobs.loc[jobs.job.str.contains(prod_str, case = False), 'Product'] = 1\n",
    "\n",
    "# jobs['Sales/Business Development/Growth'] = 0\n",
    "# sales_str = 'sales|business development|demand generation|deal desk|growth|sdr|revenue|expansion'\n",
    "# jobs.loc[jobs.job.str.contains(sales_str, case = False), 'Sales/Business Development/Growth'] = 1\n",
    "\n",
    "# jobs['Operations'] = 0\n",
    "# op_str = 'operations|program manager|project manager|chief of staff|strateg|general manager|scrum'\n",
    "# jobs.loc[jobs.job.str.contains(op_str, case = False), 'Operations'] = 1\n",
    "\n",
    "# jobs['Marketing'] = 0\n",
    "# marketing_str = 'marketing|growth|SEO|market research'\n",
    "# jobs.loc[jobs.job.str.contains(marketing_str, case = False), 'Marketing'] = 1\n",
    "\n",
    "# jobs['Customer Success'] = 0\n",
    "# cs_str = 'account (executive|manager)|customer success|customer (support|care|service)|implementation|Deployment Specialist|customer|client|crm|success|member experience|onboard'\n",
    "# jobs.loc[jobs.job.str.contains(cs_str, case = False), 'Customer Success'] = 1\n",
    "\n",
    "# jobs['Content Creation'] = 0\n",
    "# content_str = 'content|copywrite|communications|\\Wpr\\W|write|public relation'\n",
    "# jobs.loc[jobs.job.str.contains(content_str, case = False), 'Content Creation'] = 1\n",
    "\n",
    "# jobs['HR/People'] = 0\n",
    "# hr_str = '\\Whr\\W|talent|recruiter|sourcer|people|payroll|diversity|inclusion|compensation|benefits'\n",
    "# jobs.loc[jobs.job.str.contains(hr_str, case = False), 'HR/People'] = 1\n",
    "\n",
    "# jobs['Legal/Compliance'] = 0\n",
    "# legal_str = 'legal|compliance|counsel|Regulatory|attorney|litigation|lawyer'\n",
    "# jobs.loc[jobs.job.str.contains(legal_str, case = False), 'Legal/Compliance'] = 1\n",
    "\n",
    "# jobs['Finance/Accounting'] = 0\n",
    "# fin_str = 'finance|financial|accountant|accounting|comptroller|controller|payroll|audit|M&A|payable|corporate development|credit|investment|accounts receivable'\n",
    "# jobs.loc[jobs.job.str.contains(fin_str, case = False), 'Finance/Accounting'] = 1\n",
    "\n",
    "# jobs['Supply Chain/Logistics'] = 0\n",
    "# scl_str = 'supply chain|logistics|warehouse|inventory|buyer|procure|purchasing|acquisition'\n",
    "# jobs.loc[jobs.job.str.contains(scl_str, case = False), 'Supply Chain/Logistics'] = 1\n",
    "\n",
    "\n",
    "# jobs['Other'] = 0\n",
    "# jobs.loc[(jobs['Engineering/Technology'] == 0) & (jobs['Data'] == 0) & (jobs['Design'] == 0) & (jobs['Product'] == 0) & (jobs['Finance/Accounting'] == 0) & \n",
    "#          (jobs['Marketing'] == 0) & (jobs['Sales/Business Development/Growth'] == 0) & (jobs['Operations'] == 0) & (jobs['Customer Success'] == 0) & \n",
    "#          (jobs['Content Creation'] == 0) & (jobs['HR/People'] == 0) & (jobs['Legal/Compliance'] == 0) & (jobs['Supply Chain/Logistics'] == 0), 'Other'] = 1\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "source": [
    "# jobs = jobs[~jobs.company.isin(['Beacon', 'Canopy', 'DCM', 'harbor'])]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "source": [
    "# len(jobs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "source": [
    "# jobs_grp = jobs.groupby(['company', 'job_page'])[[col for col in jobs.columns if col not in ['company', 'job', 'location', 'url']]].sum()\n",
    "# jobs_grp = jobs_grp.reset_index()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "source": [
    "# jobs_grp"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "source": [
    "# jobs_grp.loc[jobs_grp.company == 'vgs', 'company'] = 'Very Good Security'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "source": [
    "# jobs_grp.loc[jobs_grp.company == 'joinclubhouse', 'company'] = 'Clubhouse'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "source": [
    "# final = pd.merge(vc_deals, jobs_grp, on = 'company', how = 'inner', copy = False)\n",
    "\n",
    "# final = pd.merge(vc_deals, [], on = 'company', how = 'inner', copy = False)\n",
    "# final = final.reset_index(drop = True)\n",
    "\n",
    "final = vc_deals"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "source": [
    "# final = final[final.company != 'DISCO']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "source": [
    "import math\n",
    "def milli_billi(n):\n",
    "    # get number of digits before the first number\n",
    "    digits = math.floor(math.log10(n)) \n",
    "    \n",
    "    if digits <= 5: #if < million, use thousands as unit\n",
    "        leading_n = round(n / 1e3, 2)\n",
    "        new_string = '$' + str(leading_n).replace('.0', '') + 'K'\n",
    "        return new_string\n",
    "    elif digits > 5 and digits <= 8: #if in the million range, use millions as units\n",
    "        leading_n = round(n / 1e6, 2)\n",
    "        new_string = '$' + str(leading_n).replace('.0', '') + ' Million'\n",
    "        return new_string\n",
    "    else: #if greater than millions, use billions\n",
    "        leading_n = round(n / 1e9, 2)\n",
    "        new_string = '$' + str(leading_n).replace('.0', '') + ' Billion'\n",
    "        return new_string"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "source": [
    "#make deal size in millions or billions\n",
    "final['deal_size_text'] = final.deal_size.apply(milli_billi, 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "source": [
    "final.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>deal_text</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>story_url</th>\n",
       "      <th>company_url</th>\n",
       "      <th>deal_size</th>\n",
       "      <th>deal_size_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Devoted Health</td>\n",
       "      <td>Devoted Health, a Waltham, Mass.-based health ...</td>\n",
       "      <td>2021-09-24</td>\n",
       "      <td>http://axios.link/9ly4</td>\n",
       "      <td>http://axios.link/9ly4</td>\n",
       "      <td>1.200000e+09</td>\n",
       "      <td>$1.2 Billion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>PandaDoc</td>\n",
       "      <td>PandaDoc, an SF-based e-document company, rais...</td>\n",
       "      <td>2021-09-23</td>\n",
       "      <td>http://axios.link/qdcX</td>\n",
       "      <td>https://www.pandadoc.com/</td>\n",
       "      <td>1.000000e+09</td>\n",
       "      <td>$1 Billion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Ninja</td>\n",
       "      <td>Ninja Van, a Singapore-based package delivery ...</td>\n",
       "      <td>2021-09-27</td>\n",
       "      <td>http://axios.link/1N5N</td>\n",
       "      <td>http://axios.link/1N5N</td>\n",
       "      <td>5.780000e+08</td>\n",
       "      <td>$578 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meesho</td>\n",
       "      <td>Meesho, an Indian social e-commerce company, r...</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>http://axios.link/9w8i</td>\n",
       "      <td>http://axios.link/9w8i</td>\n",
       "      <td>5.700000e+08</td>\n",
       "      <td>$570 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Atome</td>\n",
       "      <td>Atome, a Singapore-based BNPL startup, raised ...</td>\n",
       "      <td>2021-09-23</td>\n",
       "      <td>http://axios.link/qxF6</td>\n",
       "      <td>http://axios.link/qxF6</td>\n",
       "      <td>4.000000e+08</td>\n",
       "      <td>$400 Million</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            company                                          deal_text  \\\n",
       "99   Devoted Health  Devoted Health, a Waltham, Mass.-based health ...   \n",
       "138        PandaDoc  PandaDoc, an SF-based e-document company, rais...   \n",
       "83            Ninja  Ninja Van, a Singapore-based package delivery ...   \n",
       "0            Meesho  Meesho, an Indian social e-commerce company, r...   \n",
       "112           Atome  Atome, a Singapore-based BNPL startup, raised ...   \n",
       "\n",
       "    publish_date               story_url                company_url  \\\n",
       "99    2021-09-24  http://axios.link/9ly4     http://axios.link/9ly4   \n",
       "138   2021-09-23  http://axios.link/qdcX  https://www.pandadoc.com/   \n",
       "83    2021-09-27  http://axios.link/1N5N     http://axios.link/1N5N   \n",
       "0     2021-09-30  http://axios.link/9w8i     http://axios.link/9w8i   \n",
       "112   2021-09-23  http://axios.link/qxF6     http://axios.link/qxF6   \n",
       "\n",
       "        deal_size deal_size_text  \n",
       "99   1.200000e+09   $1.2 Billion  \n",
       "138  1.000000e+09     $1 Billion  \n",
       "83   5.780000e+08   $578 Million  \n",
       "0    5.700000e+08   $570 Million  \n",
       "112  4.000000e+08   $400 Million  "
      ]
     },
     "metadata": {},
     "execution_count": 297
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "source": [
    "len(final)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "metadata": {},
     "execution_count": 298
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "source": [
    "#check previous posts, check to see there are no duplicates\n",
    "old = pd.read_csv('historical_deals.csv')\n",
    "dupes = pd.merge(final, old[['company', 'deal_size']], how = 'inner', on = ['company', 'deal_size'])\n",
    "final = final[~final.company.isin(dupes.company)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "source": [
    "len(final)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 300
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "source": [
    "final = final[final.company != 'Bolt']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "source": [
    "final"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>deal_text</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>story_url</th>\n",
       "      <th>company_url</th>\n",
       "      <th>deal_size</th>\n",
       "      <th>deal_size_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GrayMatter Robotics</td>\n",
       "      <td>GrayMatter Robotics, an LA-based maker of manu...</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>http://www.graymatter-robotics.com/</td>\n",
       "      <td>http://www.graymatter-robotics.com/</td>\n",
       "      <td>4100000.0</td>\n",
       "      <td>$4.1 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>RunX</td>\n",
       "      <td>RunX, an SF-based cloud infrastructure deploym...</td>\n",
       "      <td>2021-09-29</td>\n",
       "      <td>http://axios.link/DRIR</td>\n",
       "      <td>https://www.runx.dev/</td>\n",
       "      <td>4100000.0</td>\n",
       "      <td>$4.1 Million</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                company                                          deal_text  \\\n",
       "24  GrayMatter Robotics  GrayMatter Robotics, an LA-based maker of manu...   \n",
       "51                 RunX  RunX, an SF-based cloud infrastructure deploym...   \n",
       "\n",
       "   publish_date                            story_url  \\\n",
       "24   2021-09-30  http://www.graymatter-robotics.com/   \n",
       "51   2021-09-29               http://axios.link/DRIR   \n",
       "\n",
       "                            company_url  deal_size deal_size_text  \n",
       "24  http://www.graymatter-robotics.com/  4100000.0   $4.1 Million  \n",
       "51                https://www.runx.dev/  4100000.0   $4.1 Million  "
      ]
     },
     "metadata": {},
     "execution_count": 302
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "source": [
    "#save deal records\n",
    "new_old = pd.concat([final, old], axis = 0)\n",
    "\n",
    "new_old.to_csv('historical_deals.csv', index = False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "source": [
    "len(new_old)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2019"
      ]
     },
     "metadata": {},
     "execution_count": 304
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "source": [
    "date = 'Sunday, 08/29/2021'\n",
    "\n",
    "title = '''\n",
    "<h1>\n",
    "  Funded & Hiring\n",
    "</h1>\n",
    "<h2 style=\"text-align:center\">\n",
    "  <strong>{date}</strong>\n",
    "</h2>\n",
    "<h2>\n",
    "  A weekly roundup of funded startups and the jobs they're hiring for.\n",
    "</h2>\n",
    "<h4>\n",
    "  Forwarded from a friend? \n",
    "  <a href = 'https://fundedandhiring.com/'>Subscribe</a> to stay up to date on the latest startup funding and job alerts.\n",
    "</h4>\n",
    "'''.format(date = date).strip()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "source": [
    "print(title)\n",
    "pyperclip.copy(title)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<h1>\n",
      "  Funded & Hiring\n",
      "</h1>\n",
      "<h2 style=\"text-align:center\">\n",
      "  <strong>Sunday, 08/29/2021</strong>\n",
      "</h2>\n",
      "<h2>\n",
      "  A weekly roundup of funded startups and the jobs they're hiring for.\n",
      "</h2>\n",
      "<h4>\n",
      "  Forwarded from a friend? \n",
      "  <a href = 'https://fundedandhiring.com/'>Subscribe</a> to stay up to date on the latest startup funding and job alerts.\n",
      "</h4>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "source": [
    "content = ''''''\n",
    "\n",
    "final_dict = final.to_dict(orient = 'records')\n",
    "for row in final_dict:\n",
    "    headline = \"HEADLINE\"\n",
    "    company_name = row['company']\n",
    "    story_url = row['story_url']\n",
    "    description = row['deal_text']\n",
    "    company_url = row['company_url']\n",
    "    \n",
    "    #get job types and job count\n",
    "    # job_types = [key for key in row.keys() if key not in ['company', 'deal_text', 'publish_date', 'story_url', 'deal_size', 'job_page', 'deal_size_text', 'company_url']]\n",
    "    # job_types_dict = {}\n",
    "    # for typ in job_types:\n",
    "    #     if row[typ] == 0:\n",
    "    #         pass\n",
    "    #     else:\n",
    "    #         job_types_dict[typ] = row[typ]\n",
    "            \n",
    "    # job_types_dict = dict(sorted(job_types_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    #order job types by number of jobs, move Other to the end\n",
    "    # job_type_order = list(job_types_dict.keys())\n",
    "    # if 'Other' in job_type_order:\n",
    "    #     job_type_order.append('Other')\n",
    "    #     del job_type_order[job_type_order.index('Other')]\n",
    "    \n",
    "    hiring_for = ''''''\n",
    "    # for key in job_type_order:\n",
    "    #     job_string = key + '({count})'.format(count = job_types_dict[key]) + ', '\n",
    "    #     hiring_for += job_string\n",
    "    # hiring_for = re.sub(r', $', '', hiring_for)\n",
    "            \n",
    "    \n",
    "    entry = '''\n",
    "    <tr>\n",
    "      <td>\n",
    "        <h3>\n",
    "          <a href = \"{story_url}\">{headline}</a>\n",
    "        </h3>\n",
    "        <h4>\n",
    "          <a href = \"{company_url}\">{company_name}</a>\n",
    "        <strong>Funding Amount</strong>: <em><strong><a href = \"{story_url}\">{deal_size_text}</a></strong></em>\n",
    "        <p>\n",
    "          {description}\n",
    "        </p>\n",
    "        <p>\n",
    "          <strong>Hiring For:</strong>\n",
    "            <a href = \"www.google.com\">\n",
    "              {hiring_for}\n",
    "            </a>\n",
    "        </p>\n",
    "      </td>\n",
    "    </tr> '''.format(story_url = story_url, company_name = company_name, deal_size_text = deal_size_text,deal_text = deal_text, job_page = 'www.google.com', hiring_for = hiring_for, company_url = company_url).strip()\n",
    "\n",
    "    content += '\\n' + entry.strip()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "source": [
    "pyperclip.copy(content)\n",
    "print(content)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "<tr>\n",
      "      <td>\n",
      "        <h3>\n",
      "          <a href = \"http://www.graymatter-robotics.com/\">GrayMatter Robotics</a>\n",
      "        </h3>\n",
      "        <strong>Funding Amount</strong>: <em><strong><a href = \"http://www.graymatter-robotics.com/\">$4.1 Million</a></strong></em>\n",
      "        <p>\n",
      "          GrayMatter Robotics, an LA-based maker of manufacturing robots, raised 4.1 million in seed funding. Stage Venture Partners and Calibrate Ventures co-led, and were joined by 3M Ventures, OCA Ventures, Pathbreaker Ventures and B Capital Group.\n",
      "        </p>\n",
      "        <p>\n",
      "          <strong>Hiring For:</strong>\n",
      "            <a href = \"www.google.com\">\n",
      "              \n",
      "            </a>\n",
      "        </p>\n",
      "      </td>\n",
      "    </tr>\n",
      "<tr>\n",
      "      <td>\n",
      "        <h3>\n",
      "          <a href = \"https://www.runx.dev/\">RunX</a>\n",
      "        </h3>\n",
      "        <strong>Funding Amount</strong>: <em><strong><a href = \"http://axios.link/DRIR\">$4.1 Million</a></strong></em>\n",
      "        <p>\n",
      "          RunX, an SF-based cloud infrastructure deployment startup, raised 4.1 million in seed funding led by Unusual Ventures.\n",
      "        </p>\n",
      "        <p>\n",
      "          <strong>Hiring For:</strong>\n",
      "            <a href = \"www.google.com\">\n",
      "              \n",
      "            </a>\n",
      "        </p>\n",
      "      </td>\n",
      "    </tr>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "source": [
    "#job boards\n",
    "# stackoverflow, jobvite, bamboohr"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}