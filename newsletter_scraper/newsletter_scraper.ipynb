{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "chromedriver = \"/Users/stevenwang/Projects/funded-and-hiring-site/newsletter_scraper/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "import pyperclip\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pytest_timeout"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "def get_ttnews():\n",
    "    url = 'https://www.ttnews.com/articles'\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    # news = []\n",
    "    companies = []\n",
    "    deal_text = []\n",
    "    publish_date = []\n",
    "    company_urls = []\n",
    "    stories = []\n",
    "    \n",
    "    articles = soup.find_all(class_='views-row')\n",
    "    # print(articles)\n",
    "    for article in articles:\n",
    "        # print(article)\n",
    "        title = article.find('a')\n",
    "        date =  article.find(class_='date-display-single')\n",
    "        text =  article.find('p')\n",
    "\n",
    "        companies.append('TTNews')\n",
    "        deal_text.append(text)\n",
    "        publish_date.append(date)\n",
    "        stories.append(title.get('href'))\n",
    "        company_urls.append('https://ttnews.com/')\n",
    "        \n",
    "        # print(title)\n",
    "        # print(title.get('href'))\n",
    "        # print(date.get_text())\n",
    "        # print(text.get_text())\n",
    "        # , date.get_text(), text.get_text())\n",
    "\n",
    "        # print(title)\n",
    "        # news.append(News(title.get_text(), title.get('href')))\n",
    "\n",
    "    dataframe = pd.DataFrame({'company':companies, 'deal_text':deal_text, 'publish_date':publish_date, 'story_url':stories, 'company_url':company_urls})\n",
    "    return dataframe"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "ttnews = get_ttnews()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "def get_axios_deals(days_back):\n",
    "    url = 'https://www.axios.com/authors/danprimack/newsletters'\n",
    "#     url = 'https://www.axios.com/authors/kiakokalitcheva/newsletters'\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    companies = []\n",
    "    deal_text = []\n",
    "    publish_date = []\n",
    "    stories = []\n",
    "    company_urls = []\n",
    "\n",
    "    #grab last 5 edition urls, check to make sure they are all within the last week\n",
    "    news = soup.find_all('amp-layout')[:days_back]\n",
    "    for letter in news:\n",
    "        #check if date of newsletter is in last week, if not then pass\n",
    "        date = letter.find_next('span', attrs = {'data-testid':'time-rubric'}).text\n",
    "        if 'hours' in date or 'mins' in date or 'hour' in date:\n",
    "            date = pd.to_datetime(datetime.now().date())\n",
    "        else:\n",
    "            date = pd.to_datetime(date)\n",
    "        if date < pd.to_datetime(datetime.now().date()) - timedelta(days = days_back):\n",
    "            break\n",
    "\n",
    "        #get company names and deal texts mentioned in each news letter\n",
    "        letter_url = letter.find_next('a', href = True, class_ = 'title-link gtm-content-click')['href']\n",
    "        letter_companies, letter_deal_text, letter_story_urls = get_axios_deal_text(letter_url)\n",
    "\n",
    "        #append these to the running list\n",
    "        companies += letter_companies\n",
    "        deal_text += letter_deal_text\n",
    "        publish_date += [date for i in range(len(letter_companies))]\n",
    "        stories += letter_story_urls\n",
    "    \n",
    "    #grab company site urls\n",
    "    for company, story in zip(companies, stories):\n",
    "        company = company.lower().strip()\n",
    "        if company in story: #if company name is in story url, then use story url as company url\n",
    "            company_urls.append(story)\n",
    "        else:#grab company url from story\n",
    "            try:\n",
    "                company_site = get_company_url_from_axios_story(company, story)\n",
    "                company_urls.append(company_site)\n",
    "            except: #if company url can't be found in the story, use story url instead\n",
    "                company_urls.append(story)\n",
    "                \n",
    "    dataframe = pd.DataFrame({'company':companies, 'deal_text':deal_text, 'publish_date':publish_date, 'story_url':stories, 'company_url':company_urls})\n",
    "    return dataframe\n",
    "\n",
    "def get_axios_deal_text(link):\n",
    "    prorata_soup =  BeautifulSoup(requests.get(link).content, \"html.parser\")\n",
    "    vc_deals = prorata_soup.find('div', text = re.compile('Venture Capital Deals')).find_next('div', class_ = re.compile('.*story-text.*')).find_all('p')\n",
    "    \n",
    "    companies = []\n",
    "    deal_text = []\n",
    "    stories = []\n",
    "\n",
    "    for deal in vc_deals:\n",
    "        company_name = deal.find_next('strong').text.strip()\n",
    "        if company_name == 'â€¢':\n",
    "            company_name = deal.find_next('strong').find_next('strong').text.strip()\n",
    "\n",
    "        text = deal.text\n",
    "        text = re.sub(r'[^-a-zA-z0-9.,!? ]', '', text).strip()\n",
    "        story_url = deal.find_next('a', class_ = 'gtm-content-click')['href']\n",
    "\n",
    "        companies.append(company_name)\n",
    "        deal_text.append(text)\n",
    "        stories.append(story_url)\n",
    "    return(companies, deal_text, stories)\n",
    "\n",
    "def get_company_url_from_axios_story(company_name, story_url):\n",
    "    page = requests.get(story_url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    article_url = soup.find('meta', attrs={'property': 'twitter:url'})['content']\n",
    "    headers = {'user-agent':'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Mobile Safari/537.36'}\n",
    "    article_page = requests.get(article_url, headers = headers)\n",
    "    article_soup = BeautifulSoup(article_page.content, \"html.parser\") \n",
    "\n",
    "    potential_company_urls = []\n",
    "    #get hyperlink url that contains company name\n",
    "    company_name_no_space = company_name.replace(' ', '')\n",
    "    company_name_no_space_url = article_soup.find_all('a', href = re.compile('.*{company_name_no_space}'.format(company_name_no_space = company_name_no_space), re.IGNORECASE))\n",
    "    if len(company_name_no_space_url) > 0:\n",
    "        potential_company_urls.append(company_name_no_space_url[0]['href'])\n",
    "    \n",
    "    company_name_hyphen = company_name.replace(' ', '-')\n",
    "    company_name_hyphen_url = article_soup.find_all('a', href = re.compile('.*{company_name_hyphen}'.format(company_name_hyphen = company_name_hyphen), re.IGNORECASE))\n",
    "    if len(company_name_hyphen_url) > 0:\n",
    "        potential_company_urls.append(company_name_hyphen_url[0]['href'])\n",
    "        \n",
    "    company_name_url = article_soup.find_all('a', text = re.compile('.*{company_name}.*'.format(company_name = company_name), re.IGNORECASE))\n",
    "    if len(company_name_url)  > 0:\n",
    "        potential_company_urls.append(company_name_url[0]['href'])\n",
    "    \n",
    "    #return the shortest url\n",
    "    return min(potential_company_urls, key=len)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "def clean_axios_deals(deals):\n",
    "    #get deal size\n",
    "    deals['deal_size'] = deals.deal_text.str.extract(r'((?:\\d+[.]\\d+|\\d+|\\d+[.]\\d+ |\\d+ )(?:million|billion|Million|Billion))')\n",
    "    \n",
    "    #only keep deals that have dollar amounts\n",
    "    deals = deals[deals.deal_size.notnull()]\n",
    "    \n",
    "    #convert to an actual number\n",
    "    deals.loc[deals.deal_size.str.contains('million|m$', case = False), 'deal_size'] = deals.deal_size.str.split(' ').str[0].astype(float) * 1e6\n",
    "    deals['deal_size'] = deals.deal_size.astype(str)\n",
    "    deals.loc[deals.deal_size.str.contains('billion|b$', case = False), 'deal_size'] = deals.deal_size.str.split(' ').str[0].astype(float) * 1e9\n",
    "    \n",
    "    #remove urls from deal text\n",
    "    deals['deal_text'] = deals.deal_text.str.split('www[.]|http').str[0].str.strip()\n",
    "    return deals"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "# axios = get_axios_deals(7)\n",
    "\n",
    "# axios\n",
    "\n",
    "# axios = clean_axios_deals(axios)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "def get_finsme_deals(days_back):\n",
    "    page = 1\n",
    "    past_date = False\n",
    "\n",
    "    companies = []\n",
    "    deal_text = []\n",
    "    publish_date = []\n",
    "    stories = []\n",
    "    company_urls = []\n",
    "    while page <= days_back and past_date == False:\n",
    "        url = 'http://www.finsmes.com/category/usa/page/' + str(page)\n",
    "        soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "        deals = soup.find_all('article')\n",
    "        for d in deals:\n",
    "            #check if date of article is > 7 days ago, if true stop\n",
    "            date = pd.to_datetime(d.find_next('time', attrs = {'itemprop':'datePublished'}).text)\n",
    "            if date < pd.to_datetime(datetime.now().date()) - timedelta(days = days_back):\n",
    "                past_date = True\n",
    "                break\n",
    "            else:\n",
    "                text = d.find_next('p').text.strip().replace('\\n', '')\n",
    "                company_name = text.split(',')[0].strip()\n",
    "                story_url = d.find('a', attrs = {'itemprop':'url'})['href']\n",
    "\n",
    "                companies.append(company_name)\n",
    "                deal_text.append(text)\n",
    "                publish_date.append(date)\n",
    "                stories.append(story_url)\n",
    "\n",
    "        page += 1\n",
    "    #grab company site urls\n",
    "    for company, story in zip(companies, stories):\n",
    "        company_site = get_company_url_from_finsmes_story(company, story)\n",
    "        company_urls.append(company_site)\n",
    "\n",
    "    dataframe = pd.DataFrame({'company':companies, 'deal_text':deal_text, \n",
    "                              'publish_date':publish_date, 'story_url':stories, 'company_url':company_urls})\n",
    "    return dataframe\n",
    "\n",
    "def clean_finsme_deals(deals):\n",
    "    #remove acquisitions\n",
    "    deals = deals[(~deals.company.str.contains('acquire|acquisition')) & (~deals.deal_text.str.contains('acquire|acquisition'))]\n",
    "    \n",
    "    #remove deals where company name is hard to parse\n",
    "    deals = deals[deals.company.str.split(' ').str.len() <= 5]\n",
    "    \n",
    "    #get deal size\n",
    "    deals['deal_size'] = deals.deal_text.str.extract(r'((?:\\d+[.]\\d+|\\d+)(?:million|billion|m|b|M))')\n",
    "    deals = deals[deals.deal_size.notnull()]\n",
    "    \n",
    "    deals.loc[deals.deal_size.str.contains('million|m$', case = False), 'deal_size'] = deals.deal_size.str.split('m|M').str[0].astype(float) * 1e6\n",
    "    deals['deal_size'] = deals.deal_size.astype(str)\n",
    "    deals.loc[deals.deal_size.str.contains('billion|b$', case = False), 'deal_size'] = deals.deal_size.str.split('b|B').str[0].astype(float) * 1e9\n",
    "    \n",
    "    \n",
    "    return deals\n",
    "\n",
    "def get_company_url_from_finsmes_story(company_name, story_url):\n",
    "    headers = {'user-agent':'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Mobile Safari/537.36'}\n",
    "    article_page = requests.get(story_url, headers = headers)\n",
    "    article_soup = BeautifulSoup(article_page.content, \"html.parser\") \n",
    "    article_content = article_soup.find('div', class_ = 'entry-content')\n",
    "    try:\n",
    "        company_url = article_content.find_all('a', text = re.compile('.*{company_name}.*'.format(company_name = company_name), re.IGNORECASE))[0]['href']\n",
    "    except:\n",
    "        company_url = story_url\n",
    "    return company_url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "# finsme = get_finsme_deals(7)\n",
    "# finsme = clean_finsme_deals(finsme)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "vc_deals = pd.concat([ttnews], axis = 0)\n",
    "#replace all .coms in company names\n",
    "vc_deals['company'] = vc_deals.company.str.replace(\"[.].*|â€¢|â€™|,|[+]|[()]|[#]\", '').str.strip()\n",
    "\n",
    "#remove unicode spaces\n",
    "# vc_deals['company'] = vc_deals.company.str.replace(u'\\xa0|\\u200d', ' ').str.strip()\n",
    "# vc_deals['company'] = vc_deals.company.str.replace('&', 'and').str.strip()\n",
    "# vc_deals['company'] = vc_deals.company.str.replace('[/]', '').str.strip()\n",
    "\n",
    "# vc_deals.loc[vc_deals.company == 'ðŸ§€', 'company'] = 'Nobell Foods'\n",
    "# vc_deals['company'] = vc_deals.company.str.replace('â˜•', '')\n",
    "# vc_deals['company'] = vc_deals.company.str.replace('ðŸš™', '')\n",
    "# vc_deals['company'] = vc_deals.company.str.replace('ðŸš‘', '')\n",
    "# vc_deals['company'] = vc_deals.company.str.replace('ðŸ¶', '')\n",
    "# vc_deals['company'] = vc_deals.company.str.replace('â›½ï¸|ðŸ‰', '')\n",
    "\n",
    "\n",
    "# vc_deals['company'] = vc_deals.company.str.strip()\n",
    "# vc_deals = vc_deals[vc_deals.company != '']\n",
    "# vc_deals = vc_deals[vc_deals.company != 'Vestiaire Collective']\n",
    "\n",
    "\n",
    "\n",
    "# vc_deals = vc_deals[~vc_deals.company.str.upper().duplicated()]\n",
    "# vc_deals['deal_size'] = vc_deals.deal_size.astype(float)\n",
    "# vc_deals = vc_deals.sort_values('deal_size', ascending = False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "len(vc_deals)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "vc_deals"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>deal_text</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>story_url</th>\n",
       "      <th>company_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ Lordstown Motors Corp., the electric-truck m...</td>\n",
       "      <td>[September 30, 2021]</td>\n",
       "      <td>/articles/lordstown-nears-deal-sell-ohio-plant...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ ABB Ltd. is preparing to roll out the first ...</td>\n",
       "      <td>[September 30, 2021]</td>\n",
       "      <td>/articles/new-ev-charger-can-provide-62-miles-...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ After sounding an optimistic note in the fir...</td>\n",
       "      <td>[September 30, 2021]</td>\n",
       "      <td>/articles/car-sales-drop-chip-shortages-stymie...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ WASHINGTON â€”Â His government overhaul plans a...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/progress-budget-talks-will-deal-beat...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ The outlook for transportation and logistics...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/what-q4-and-beyond-have-store-transp...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ LONDON â€” The British governmentâ€™s reserve ta...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/soldiers-will-haul-fuel-ease-uk-crisis</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ Navistar Inc. announced the launch of the re...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/navistar-launches-diesel-version-lat...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ Legal experts and researchers say juries gen...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/negative-jury-attitudes-can-contribu...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ Walmart Inc. is expanding its hiring push wi...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/walmart-stokes-hiring-rush-plan-add-...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ When the first snub-nosed, electric van roll...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/electric-vans-roll-line-once-made-ga...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ Loadsmart launched a platform in partnership...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/loadsmart-home-depot-launch-supply-l...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ As automakers scramble to make electric vehi...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/silicon-valley-answer-ev-question-ca...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ Online shoppers will be buying less this hol...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/inflation-seen-pushing-online-holida...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ WASHINGTON â€” Pressure mounting, President Jo...</td>\n",
       "      <td>[September 28, 2021]</td>\n",
       "      <td>/articles/big-pressure-biden-dems-trim-35t-fed...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ Lucid Group Inc. has started production on i...</td>\n",
       "      <td>[September 28, 2021]</td>\n",
       "      <td>/articles/lucid-deliver-520-mile-electric-seda...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ WASHINGTON â€” Treasury Secretary Janet Yellen...</td>\n",
       "      <td>[September 28, 2021]</td>\n",
       "      <td>/articles/yellen-warns-delay-raising-debt-limi...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ General Motors Co.â€™s electric van unit plans...</td>\n",
       "      <td>[September 28, 2021]</td>\n",
       "      <td>/articles/gm-debuts-midsize-electric-delivery-van</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ LONDON â€” Long lines are snaking down streets...</td>\n",
       "      <td>[September 28, 2021]</td>\n",
       "      <td>/articles/why-and-how-uk-experiencing-fuel-crisis</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ A highway project vital to the flow of freig...</td>\n",
       "      <td>[September 28, 2021]</td>\n",
       "      <td>/articles/colorados-central-70-project-receive...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ Commerce Secretary Gina Raimondo made a fres...</td>\n",
       "      <td>[September 28, 2021]</td>\n",
       "      <td>/articles/raimondo-says-ceos-more-reasonable-t...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>/articles/loadsmart-home-depot-launch-supply-l...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>/articles/big-pressure-biden-dems-trim-35t-fed...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>/articles/missouri-lawmakers-pass-fuel-tax-hik...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>/articles/usps-mail-small-package-delivery-may...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>/articles/lucid-deliver-520-mile-electric-seda...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   company                                          deal_text  \\\n",
       "0   TTNews  [ Lordstown Motors Corp., the electric-truck m...   \n",
       "1   TTNews  [ ABB Ltd. is preparing to roll out the first ...   \n",
       "2   TTNews  [ After sounding an optimistic note in the fir...   \n",
       "3   TTNews  [ WASHINGTON â€”Â His government overhaul plans a...   \n",
       "4   TTNews  [ The outlook for transportation and logistics...   \n",
       "5   TTNews  [ LONDON â€” The British governmentâ€™s reserve ta...   \n",
       "6   TTNews  [ Navistar Inc. announced the launch of the re...   \n",
       "7   TTNews  [ Legal experts and researchers say juries gen...   \n",
       "8   TTNews  [ Walmart Inc. is expanding its hiring push wi...   \n",
       "9   TTNews  [ When the first snub-nosed, electric van roll...   \n",
       "10  TTNews  [ Loadsmart launched a platform in partnership...   \n",
       "11  TTNews  [ As automakers scramble to make electric vehi...   \n",
       "12  TTNews  [ Online shoppers will be buying less this hol...   \n",
       "13  TTNews  [ WASHINGTON â€” Pressure mounting, President Jo...   \n",
       "14  TTNews  [ Lucid Group Inc. has started production on i...   \n",
       "15  TTNews  [ WASHINGTON â€” Treasury Secretary Janet Yellen...   \n",
       "16  TTNews  [ General Motors Co.â€™s electric van unit plans...   \n",
       "17  TTNews  [ LONDON â€” Long lines are snaking down streets...   \n",
       "18  TTNews  [ A highway project vital to the flow of freig...   \n",
       "19  TTNews  [ Commerce Secretary Gina Raimondo made a fres...   \n",
       "20  TTNews                                               None   \n",
       "21  TTNews                                               None   \n",
       "22  TTNews                                               None   \n",
       "23  TTNews                                               None   \n",
       "24  TTNews                                               None   \n",
       "\n",
       "            publish_date                                          story_url  \\\n",
       "0   [September 30, 2021]  /articles/lordstown-nears-deal-sell-ohio-plant...   \n",
       "1   [September 30, 2021]  /articles/new-ev-charger-can-provide-62-miles-...   \n",
       "2   [September 30, 2021]  /articles/car-sales-drop-chip-shortages-stymie...   \n",
       "3   [September 29, 2021]  /articles/progress-budget-talks-will-deal-beat...   \n",
       "4   [September 29, 2021]  /articles/what-q4-and-beyond-have-store-transp...   \n",
       "5   [September 29, 2021]   /articles/soldiers-will-haul-fuel-ease-uk-crisis   \n",
       "6   [September 29, 2021]  /articles/navistar-launches-diesel-version-lat...   \n",
       "7   [September 29, 2021]  /articles/negative-jury-attitudes-can-contribu...   \n",
       "8   [September 29, 2021]  /articles/walmart-stokes-hiring-rush-plan-add-...   \n",
       "9   [September 29, 2021]  /articles/electric-vans-roll-line-once-made-ga...   \n",
       "10  [September 29, 2021]  /articles/loadsmart-home-depot-launch-supply-l...   \n",
       "11  [September 29, 2021]  /articles/silicon-valley-answer-ev-question-ca...   \n",
       "12  [September 29, 2021]  /articles/inflation-seen-pushing-online-holida...   \n",
       "13  [September 28, 2021]  /articles/big-pressure-biden-dems-trim-35t-fed...   \n",
       "14  [September 28, 2021]  /articles/lucid-deliver-520-mile-electric-seda...   \n",
       "15  [September 28, 2021]  /articles/yellen-warns-delay-raising-debt-limi...   \n",
       "16  [September 28, 2021]  /articles/gm-debuts-midsize-electric-delivery-van   \n",
       "17  [September 28, 2021]  /articles/why-and-how-uk-experiencing-fuel-crisis   \n",
       "18  [September 28, 2021]  /articles/colorados-central-70-project-receive...   \n",
       "19  [September 28, 2021]  /articles/raimondo-says-ceos-more-reasonable-t...   \n",
       "20                  None  /articles/loadsmart-home-depot-launch-supply-l...   \n",
       "21                  None  /articles/big-pressure-biden-dems-trim-35t-fed...   \n",
       "22                  None  /articles/missouri-lawmakers-pass-fuel-tax-hik...   \n",
       "23                  None  /articles/usps-mail-small-package-delivery-may...   \n",
       "24                  None  /articles/lucid-deliver-520-mile-electric-seda...   \n",
       "\n",
       "            company_url  \n",
       "0   https://ttnews.com/  \n",
       "1   https://ttnews.com/  \n",
       "2   https://ttnews.com/  \n",
       "3   https://ttnews.com/  \n",
       "4   https://ttnews.com/  \n",
       "5   https://ttnews.com/  \n",
       "6   https://ttnews.com/  \n",
       "7   https://ttnews.com/  \n",
       "8   https://ttnews.com/  \n",
       "9   https://ttnews.com/  \n",
       "10  https://ttnews.com/  \n",
       "11  https://ttnews.com/  \n",
       "12  https://ttnews.com/  \n",
       "13  https://ttnews.com/  \n",
       "14  https://ttnews.com/  \n",
       "15  https://ttnews.com/  \n",
       "16  https://ttnews.com/  \n",
       "17  https://ttnews.com/  \n",
       "18  https://ttnews.com/  \n",
       "19  https://ttnews.com/  \n",
       "20  https://ttnews.com/  \n",
       "21  https://ttnews.com/  \n",
       "22  https://ttnews.com/  \n",
       "23  https://ttnews.com/  \n",
       "24  https://ttnews.com/  "
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "# def get_lever_jobs(companies):\n",
    "#     company_ls = []\n",
    "#     job_ls = []\n",
    "#     location_ls = []\n",
    "#     listing_url = []\n",
    "#     no_lever = []\n",
    "\n",
    "#     for company in companies:\n",
    "#         company_cleaned = company.replace(' ', '').lower()\n",
    "#         url = 'https://jobs.lever.co/' + company_cleaned\n",
    "#         soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "#         jobs = soup.find_all('div', class_ = 'posting')\n",
    "\n",
    "#         #if no jobs found, add to no lever list\n",
    "#         if len(jobs) == 0:\n",
    "#             no_lever.append(company)\n",
    "#         else:\n",
    "#             for job in jobs:\n",
    "#                 job_name = job.find('h5', attrs = {'data-qa':'posting-name'}).text.strip()\n",
    "\n",
    "#                 job_location = job.find('span', attrs = {'class':'sort-by-location posting-category small-category-label'})\n",
    "#                 if job_location == None: #not all jobs have location, set to none if missing\n",
    "#                     job_location = None\n",
    "#                 else:\n",
    "#                     job_location = job_location.text.strip()\n",
    "\n",
    "#                 company_ls.append(company)\n",
    "#                 job_ls.append(job_name)\n",
    "#                 location_ls.append(job_location)\n",
    "#                 listing_url.append(url)\n",
    "#     df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})\n",
    "    \n",
    "#     print(len(companies) - len(no_lever), 'out of ', len(companies), 'companies found on Lever')\n",
    "#     return df, no_lever\n",
    "    "
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "# lever_jobs, not_on_lever = get_lever_jobs(vc_deals.company)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "# import signal\n",
    "\n",
    "# class Timeout:\n",
    "#     def __init__(self, seconds=1, error_message='Timeout'):\n",
    "#         self.seconds = seconds\n",
    "#         self.error_message = error_message\n",
    "#     def handle_timeout(self, signum, frame):\n",
    "#         raise TimeoutError(self.error_message)\n",
    "#     def __enter__(self):\n",
    "#         signal.signal(signal.SIGALRM, self.handle_timeout)\n",
    "#         signal.alarm(self.seconds)\n",
    "#     def __exit__(self, type, value, traceback):\n",
    "#         signal.alarm(0)\n",
    "\n",
    "# def test_it_doesnt_succeed():\n",
    "#     try:\n",
    "#         with Timeout(seconds=6):\n",
    "#             do_the_thing()\n",
    "#     except TimeoutError:\n",
    "#         pass\n",
    "#     else:\n",
    "#         raise AssertionError('Expected the thing to timeout!')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "# company_ls = []\n",
    "# job_ls = []\n",
    "# location_ls = []\n",
    "# listing_url = []\n",
    "# no_greenhouse = []\n",
    "\n",
    "# for company in companies:\n",
    "#     company_cleaned = company.replace(' ', '')\n",
    "#     url = 'https://boards.greenhouse.io/' + company_cleaned\n",
    "#     print(url)\n",
    "#     try:\n",
    "# #         test_it_doesnt_succeed(requests.get(url).content, 'html.parser')\n",
    "#         soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "#     except:\n",
    "#         no_greenhouse.append(company)\n",
    "#         continue\n",
    "#     jobs = soup.find_all('div', class_ = 'opening')\n",
    "\n",
    "#     #if no jobs found, add to no lever list\n",
    "#     if len(jobs) == 0:\n",
    "#         no_greenhouse.append(company)\n",
    "#     else:\n",
    "#         for job in jobs:\n",
    "#             job_name = job.find('a', attrs = {'data-mapped':\"true\"}).text.strip()\n",
    "#             job_location = job.find('span', class_ = 'location').text.strip()\n",
    "\n",
    "#             company_ls.append(company)\n",
    "#             job_ls.append(job_name)\n",
    "#             location_ls.append(job_location)\n",
    "#             listing_url.append(url)\n",
    "# df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "# def get_greenhouse_jobs(companies):\n",
    "#     company_ls = []\n",
    "#     job_ls = []\n",
    "#     location_ls = []\n",
    "#     listing_url = []\n",
    "#     no_greenhouse = []\n",
    "\n",
    "#     for company in companies:\n",
    "#         company_cleaned = company.replace(' ', '')\n",
    "#         url = 'https://boards.greenhouse.io/' + company_cleaned\n",
    "#         try:\n",
    "#             soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "#         except:\n",
    "#             no_greenhouse.append(company)\n",
    "#             continue\n",
    "#         jobs = soup.find_all('div', class_ = 'opening')\n",
    "\n",
    "#         #if no jobs found, add to no lever list\n",
    "#         if len(jobs) == 0:\n",
    "#             no_greenhouse.append(company)\n",
    "#         else:\n",
    "#             for job in jobs:\n",
    "#                 job_name = job.find('a', attrs = {'data-mapped':\"true\"}).text.strip()\n",
    "#                 job_location = job.find('span', class_ = 'location').text.strip()\n",
    "\n",
    "#                 company_ls.append(company)\n",
    "#                 job_ls.append(job_name)\n",
    "#                 location_ls.append(job_location)\n",
    "#                 listing_url.append(url)\n",
    "#     df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})\n",
    "#     print(len(companies) - len(no_greenhouse), 'out of ', len(companies), 'companies found on greenhouse')\n",
    "#     return(df, no_greenhouse)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "# greenhouse_jobs, not_on_greenhouse = get_greenhouse_jobs(not_on_lever)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "# def get_breezy_jobs(companies):\n",
    "#     headers = {'user-agent':'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Mobile Safari/537.36'}\n",
    "    \n",
    "#     company_ls = []\n",
    "#     job_ls = []\n",
    "#     location_ls = []\n",
    "#     listing_url = []\n",
    "#     no_breezy = []\n",
    "    \n",
    "#     for company in companies:\n",
    "#         company_cleaned = company.replace(' ', '').lower() \n",
    "#         url = 'https://' + company_cleaned + '.breezy.hr'\n",
    "#         soup = BeautifulSoup(requests.get(url, headers = headers).content, 'html.parser')\n",
    "        \n",
    "#         jobs = soup.find_all('li', attrs = {'class':'position transition'})\n",
    "        \n",
    "#         #if no jobs found, add to no breezy list\n",
    "#         if len(jobs) == 0:\n",
    "#             no_breezy.append(company)\n",
    "#         else:\n",
    "#             for job in jobs:\n",
    "#                 job_name = job.find_next('h2').text.strip()\n",
    "#                 job_location = job.find_next('li', attrs = {'class':'location'}).text.strip()\n",
    "                \n",
    "#                 company_ls.append(company)\n",
    "#                 job_ls.append(job_name)\n",
    "#                 location_ls.append(job_location)\n",
    "#                 listing_url.append(url)\n",
    "                \n",
    "#     df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})\n",
    "#     print(len(companies) - len(no_breezy), 'out of ', len(companies), 'companies found on breezy')\n",
    "#     return df, no_breezy\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "# breezy_jobs, not_on_breezy = get_breezy_jobs(not_on_greenhouse)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "# def get_google_jobs(companies):\n",
    "#     company_ls = []\n",
    "#     job_ls = []\n",
    "#     location_ls = []\n",
    "#     listing_url = []\n",
    "#     no_google = []\n",
    "\n",
    "#     for company in companies:\n",
    "#         company_cleaned = company.replace(' ', '').lower()\n",
    "#         url = 'https://hire.withgoogle.com/public/jobs/' + company_cleaned\n",
    "#         soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "#         jobs = soup.find_all('li', class_ = 'bb-public-jobs-list__job-item ptor-jobs-list__item')\n",
    "\n",
    "#         #if no jobs found, add to no lever list\n",
    "#         if len(jobs) == 0:\n",
    "#             no_google.append(company)\n",
    "#         else:\n",
    "#             for job in jobs:\n",
    "#                 job_name = job.find('span', class_ = 'bb-public-jobs-list__job-item-title ptor-jobs-list__item-job-title').text.strip()\n",
    "#                 job_location = job.find('span', class_ = 'bb-public-jobs-list__job-item-location ptor-jobs-list__item-location').text.strip()\n",
    "\n",
    "#                 company_ls.append(company)\n",
    "#                 job_ls.append(job_name)\n",
    "#                 location_ls.append(job_location)\n",
    "#                 listing_url.append(url)\n",
    "#     df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})\n",
    "#     print(len(companies) - len(no_google), 'out of ', len(companies), 'companies found on hirewithgoogle')\n",
    "#     return(df, no_google)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "# google_jobs, not_on_google = get_google_jobs(not_on_greenhouse)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "# def get_angel_jobs(companies):\n",
    "# #     driver = webdriver.Chrome(chromedriver)\n",
    "    \n",
    "#     company_ls = []\n",
    "#     job_ls = []\n",
    "#     location_ls = []\n",
    "#     listing_url = []\n",
    "#     no_angel = []\n",
    "\n",
    "#     for company in companies:\n",
    "#         driver = webdriver.Chrome(chromedriver)\n",
    "#         company_cleaned = company.replace(' ', '').lower()\n",
    "#         url = 'https://angel.co/company/' + company_cleaned + '/jobs'\n",
    "#         driver.get(url)\n",
    "        \n",
    "#         soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "#         jobs = soup.find_all('div', class_ = 'styles_component__1_YxE styles_expanded__31zII')\n",
    "\n",
    "#         #if no jobs found, add to no lever list\n",
    "#         if len(jobs) == 0:\n",
    "#             no_angel.append(company)\n",
    "#         else:\n",
    "#             for job in jobs:\n",
    "#                 job_name = job.find('div', class_ = 'breakpoint__desktop-up').text.strip()\n",
    "#                 job_location = job.find('div', class_ = 'styles_component__26gqE styles_truncate__dUufp styles_location__ACesY').text.strip()\n",
    "\n",
    "#                 company_ls.append(company)\n",
    "#                 job_ls.append(job_name)\n",
    "#                 location_ls.append(job_location)\n",
    "#                 listing_url.append(url)\n",
    "\n",
    "#         driver.quit()\n",
    "#     df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})\n",
    "#     print(len(companies) - len(no_angel), 'out of ', len(companies), 'companies found on angel.co')\n",
    "# #     driver.quit()\n",
    "#     return(df, no_angel)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "# angel_jobs, not_on_angel = get_angel_jobs(not_on_greenhouse)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "# def get_smartrecruiter_jobs(companies):\n",
    "#     company_ls = []\n",
    "#     job_ls = []\n",
    "#     location_ls = []\n",
    "#     listing_url = []\n",
    "#     no_smartrecruiter = []\n",
    "\n",
    "#     for company in companies:\n",
    "#         company_cleaned = company.replace(' ', '')\n",
    "#         url = 'https://careers.smartrecruiters.com/' + company_cleaned\n",
    "#         soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "\n",
    "#         locations = soup.find_all('section', class_ = 'openings-section opening opening--grouped js-group')\n",
    "        \n",
    "#         if len(locations) == 0:\n",
    "#             no_smartrecruiter.append(company)\n",
    "#         else:\n",
    "#             for loc in locations:\n",
    "#                 job_location = loc.find('h3', class_ = 'opening-title title display--inline-block text--default').text.strip()\n",
    "#                 jobs = loc.find_all('h4', class_ = 'details-title job-title link--block-target')\n",
    "#                 for job in jobs:\n",
    "#                     company_ls.append(company)\n",
    "#                     job_name = job.text.strip()\n",
    "#                     job_ls.append(job_name)\n",
    "#                     location_ls.append(job_location)\n",
    "#                     listing_url.append(url)\n",
    "                    \n",
    "#     df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})\n",
    "#     print(len(companies) - len(no_smartrecruiter), 'out of ', len(companies), 'companies found on smartrecruiter')\n",
    "#     return(df, no_smartrecruiter)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "# sr_jobs, not_on_sr = get_smartrecruiter_jobs(not_on_breezy)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "# def get_workable_jobs(companies):\n",
    "#     company_ls = []\n",
    "#     job_ls = []\n",
    "#     location_ls = []\n",
    "#     listing_url = []\n",
    "#     no_workable = []\n",
    "    \n",
    "#     for company in companies:\n",
    "#         driver = webdriver.Chrome(chromedriver)\n",
    "#         company_cleaned = company.replace(' ', '').lower()\n",
    "#         url = 'https://apply.workable.com/' + company_cleaned\n",
    "\n",
    "#         driver.get(url)\n",
    "#         try:\n",
    "#         #wait until job list shows up\n",
    "#             WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, \"//li[@data-ui='job-opening']\")))\n",
    "#         except:\n",
    "#             no_workable.append(company)\n",
    "#             driver.quit()\n",
    "#             continue\n",
    "#         soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        \n",
    "#         #click load more jobs\n",
    "#         load_more_button = soup.find_all(attrs = {'data-ui':'load-more-button'})\n",
    "#         while len(load_more_button) > 0:\n",
    "#             btn = driver.find_element_by_xpath(\"//button[@data-ui='load-more-button']\")\n",
    "#             btn.click()\n",
    "#         #     try:\n",
    "#         #         WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, \"//button[@data-ui='load-more-button']\")))\n",
    "#         #     except:\n",
    "#         #         pass\n",
    "#             time.sleep(2)\n",
    "#             soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "#             load_more_button = soup.find_all(attrs = {'data-ui':'load-more-button'})\n",
    "        \n",
    "#         jobs_list = soup.find_all('li', attrs = {'data-ui':'job-opening'})\n",
    "\n",
    "#         if len(jobs_list) == 0:\n",
    "#             no_workable.append(company)\n",
    "#         else:\n",
    "#             for job in jobs_list:\n",
    "#                 job_name = job.find_next(attrs = {'data-id':'job-item'}).text\n",
    "#                 job_url = 'https://apply.workable.com' + job.find_next('a')['href']\n",
    "#                 try:\n",
    "#                     job_location = job.find_next(attrs = {'data-ui':'job-location'}).text\n",
    "#                 except:\n",
    "#                     job_location = None\n",
    "                \n",
    "#                 #check if remote job\n",
    "#                 if len(job.find_all(attrs = {'data-ui':'job-remote'})) > 0:\n",
    "#                     job_location = 'Remote'\n",
    "\n",
    "#                 company_ls.append(company)\n",
    "#                 job_ls.append(job_name)\n",
    "#                 location_ls.append(job_location)\n",
    "#                 listing_url.append(url)\n",
    "            \n",
    "#         driver.quit()\n",
    "#     df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})\n",
    "#     print(len(companies) - len(no_workable), 'out of ', len(companies), 'companies found on workable')\n",
    "#     return(df, no_workable)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "# workable_jobs, not_on_workable = get_workable_jobs(not_on_sr)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "# driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "# url = 'https://apply.workable.com/zenoti'\n",
    "# driver.get(url)\n",
    "\n",
    "# #wait until job list shows up\n",
    "# WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//li[@data-ui='job-opening']\")))\n",
    "\n",
    "# soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# #click load more jobs\n",
    "# load_more_button = soup.find_all(attrs = {'data-ui':'load-more-button'})\n",
    "# while len(load_more_button) > 0:\n",
    "#     btn = driver.find_element_by_xpath(\"//button[@data-ui='load-more-button']\")\n",
    "#     btn.click()\n",
    "# #     try:\n",
    "# #         WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, \"//button[@data-ui='load-more-button']\")))\n",
    "# #     except:\n",
    "# #         pass\n",
    "#     time.sleep(2)\n",
    "#     soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "#     load_more_button = soup.find_all(attrs = {'data-ui':'load-more-button'})\n",
    "\n",
    "# jobs_list = soup.find_all('li', attrs = {'data-ui':'job-opening'})\n",
    "\n",
    "# for job in jobs_list:\n",
    "#     job_name = job.find_next(attrs = {'data-id':'job-item'}).text\n",
    "#     job_url = 'https://apply.workable.com' + job.find_next('a')['href']\n",
    "    \n",
    "#     job_location = job.find_next(attrs = {'data-ui':'job-location'}).text\n",
    "#     #check if remote job\n",
    "#     if len(job.find_all(attrs = {'data-ui':'job-remote'})) > 0:\n",
    "#         job_location = 'Remote'\n",
    "#     print(job_name, job_url, job_location)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "# def get_indeed_jobs(companies):\n",
    "#     company_ls = []\n",
    "#     job_ls = []\n",
    "#     location_ls = []\n",
    "#     listing_url = []\n",
    "#     no_indeed = []\n",
    "#     headers = {'user-agent':'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Mobile Safari/537.36'}\n",
    "\n",
    "\n",
    "#     for company in companies:\n",
    "#         company_cleaned = company.replace(' ', '-')\n",
    "#         url = 'https://www.indeed.com/cmp/' + company_cleaned + '/jobs?q=&l=#cmp-skip-header-desktop'\n",
    "#         soup = BeautifulSoup(requests.get(url, headers = headers).content, 'html.parser')\n",
    "#         jobs = soup.find_all('li', class_ = 'cmp-JobListItem')\n",
    "\n",
    "#         #if no jobs found, add to no lever list\n",
    "#         if len(jobs) == 0:\n",
    "#             no_indeed.append(company)\n",
    "#         else:\n",
    "#             for job in jobs:\n",
    "#                 job_name = job.find('div', class_ = 'cmp-JobListItem-title').text.strip()\n",
    "#                 job_location = job.find('div', class_ = 'cmp-JobListItem-subtitle').text.strip()\n",
    "\n",
    "#                 company_ls.append(company)\n",
    "#                 job_ls.append(job_name)\n",
    "#                 location_ls.append(job_location)\n",
    "#                 listing_url.append(url)\n",
    "#     df = pd.DataFrame({'company':company_ls, 'job':job_ls, 'location':location_ls, 'job_page':listing_url})\n",
    "#     print(len(companies) - len(no_indeed), 'out of ', len(companies), 'companies found on indeed')\n",
    "#     return(df, no_indeed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "# indeed_jobs, not_on_indeed = get_indeed_jobs(not_on_workable)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "#concat jobs\n",
    "# jobs = pd.concat([lever_jobs, greenhouse_jobs, indeed_jobs, sr_jobs, workable_jobs], axis = 0)\n",
    "# jobs = lever_jobs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "# len(jobs.company.unique())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "# jobs.job.unique().tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "#remote jobs\n",
    "# jobs['Remote'] = 0\n",
    "# jobs.loc[(jobs.location.str.contains('Remote', case = False, na = False)) | \n",
    "#          (jobs.job.str.contains('Remote', case = False, na = False)), 'Remote'] = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "#job categories\n",
    "# jobs['Engineering/Technology'] = 0\n",
    "# eng_str = 'engineer|developer|quality assurance|Technical Lead|tech lead|devops|database admin|(machine|deep) learning|scientist|\\Wqa\\W|infrastructure|\\WML\\W|tech.*lead|architect|chemist|research|geneticist|computational|informatics|scientist|\\WIT\\W|programmer|\\WML\\W|biolog|pharma|oncology|chemical|dev op|computational|cloud|Bioinformatician'\n",
    "# jobs.loc[jobs.job.str.contains(eng_str, case = False), 'Engineering/Technology'] = 1\n",
    "\n",
    "# jobs['Data'] = 0\n",
    "# data_str = 'database admin|(machine|deep) learning|scientist|data|analytics|\\WML\\W|business intelligence|BI Analyst|\\WAI\\W|product analyst|statistician|\\WBI\\W|DBA'\n",
    "# jobs.loc[jobs.job.str.contains(data_str, case = False), 'Data'] = 1\n",
    "\n",
    "# #group data into engineering/technology\n",
    "# jobs.loc[jobs.Data == 1, 'Engineering/Technology'] = 1\n",
    "\n",
    "# jobs['Design'] = 0\n",
    "# design_str = 'design|user experience|\\Wux\\W|\\Wui\\W|creative director|\\Wart\\W|graphic'\n",
    "# jobs.loc[jobs.job.str.contains(design_str, case = False), 'Design'] = 1\n",
    "\n",
    "# jobs['Product'] = 0\n",
    "# prod_str = 'product'\n",
    "# jobs.loc[jobs.job.str.contains(prod_str, case = False), 'Product'] = 1\n",
    "\n",
    "# jobs['Sales/Business Development/Growth'] = 0\n",
    "# sales_str = 'sales|business development|demand generation|deal desk|growth|sdr|revenue|expansion'\n",
    "# jobs.loc[jobs.job.str.contains(sales_str, case = False), 'Sales/Business Development/Growth'] = 1\n",
    "\n",
    "# jobs['Operations'] = 0\n",
    "# op_str = 'operations|program manager|project manager|chief of staff|strateg|general manager|scrum'\n",
    "# jobs.loc[jobs.job.str.contains(op_str, case = False), 'Operations'] = 1\n",
    "\n",
    "# jobs['Marketing'] = 0\n",
    "# marketing_str = 'marketing|growth|SEO|market research'\n",
    "# jobs.loc[jobs.job.str.contains(marketing_str, case = False), 'Marketing'] = 1\n",
    "\n",
    "# jobs['Customer Success'] = 0\n",
    "# cs_str = 'account (executive|manager)|customer success|customer (support|care|service)|implementation|Deployment Specialist|customer|client|crm|success|member experience|onboard'\n",
    "# jobs.loc[jobs.job.str.contains(cs_str, case = False), 'Customer Success'] = 1\n",
    "\n",
    "# jobs['Content Creation'] = 0\n",
    "# content_str = 'content|copywrite|communications|\\Wpr\\W|write|public relation'\n",
    "# jobs.loc[jobs.job.str.contains(content_str, case = False), 'Content Creation'] = 1\n",
    "\n",
    "# jobs['HR/People'] = 0\n",
    "# hr_str = '\\Whr\\W|talent|recruiter|sourcer|people|payroll|diversity|inclusion|compensation|benefits'\n",
    "# jobs.loc[jobs.job.str.contains(hr_str, case = False), 'HR/People'] = 1\n",
    "\n",
    "# jobs['Legal/Compliance'] = 0\n",
    "# legal_str = 'legal|compliance|counsel|Regulatory|attorney|litigation|lawyer'\n",
    "# jobs.loc[jobs.job.str.contains(legal_str, case = False), 'Legal/Compliance'] = 1\n",
    "\n",
    "# jobs['Finance/Accounting'] = 0\n",
    "# fin_str = 'finance|financial|accountant|accounting|comptroller|controller|payroll|audit|M&A|payable|corporate development|credit|investment|accounts receivable'\n",
    "# jobs.loc[jobs.job.str.contains(fin_str, case = False), 'Finance/Accounting'] = 1\n",
    "\n",
    "# jobs['Supply Chain/Logistics'] = 0\n",
    "# scl_str = 'supply chain|logistics|warehouse|inventory|buyer|procure|purchasing|acquisition'\n",
    "# jobs.loc[jobs.job.str.contains(scl_str, case = False), 'Supply Chain/Logistics'] = 1\n",
    "\n",
    "\n",
    "# jobs['Other'] = 0\n",
    "# jobs.loc[(jobs['Engineering/Technology'] == 0) & (jobs['Data'] == 0) & (jobs['Design'] == 0) & (jobs['Product'] == 0) & (jobs['Finance/Accounting'] == 0) & \n",
    "#          (jobs['Marketing'] == 0) & (jobs['Sales/Business Development/Growth'] == 0) & (jobs['Operations'] == 0) & (jobs['Customer Success'] == 0) & \n",
    "#          (jobs['Content Creation'] == 0) & (jobs['HR/People'] == 0) & (jobs['Legal/Compliance'] == 0) & (jobs['Supply Chain/Logistics'] == 0), 'Other'] = 1\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "# jobs = jobs[~jobs.company.isin(['Beacon', 'Canopy', 'DCM', 'harbor'])]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "# len(jobs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "# jobs_grp = jobs.groupby(['company', 'job_page'])[[col for col in jobs.columns if col not in ['company', 'job', 'location', 'url']]].sum()\n",
    "# jobs_grp = jobs_grp.reset_index()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "# jobs_grp"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "# jobs_grp.loc[jobs_grp.company == 'vgs', 'company'] = 'Very Good Security'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "# jobs_grp.loc[jobs_grp.company == 'joinclubhouse', 'company'] = 'Clubhouse'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "# final = pd.merge(vc_deals, jobs_grp, on = 'company', how = 'inner', copy = False)\n",
    "\n",
    "# final = pd.merge(vc_deals, [], on = 'company', how = 'inner', copy = False)\n",
    "# final = final.reset_index(drop = True)\n",
    "\n",
    "final = vc_deals"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "# final = final[final.company != 'DISCO']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "# import math\n",
    "# def milli_billi(n):\n",
    "#     # get number of digits before the first number\n",
    "#     digits = math.floor(math.log10(n)) \n",
    "    \n",
    "#     if digits <= 5: #if < million, use thousands as unit\n",
    "#         leading_n = round(n / 1e3, 2)\n",
    "#         new_string = '$' + str(leading_n).replace('.0', '') + 'K'\n",
    "#         return new_string\n",
    "#     elif digits > 5 and digits <= 8: #if in the million range, use millions as units\n",
    "#         leading_n = round(n / 1e6, 2)\n",
    "#         new_string = '$' + str(leading_n).replace('.0', '') + ' Million'\n",
    "#         return new_string\n",
    "#     else: #if greater than millions, use billions\n",
    "#         leading_n = round(n / 1e9, 2)\n",
    "#         new_string = '$' + str(leading_n).replace('.0', '') + ' Billion'\n",
    "#         return new_string"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "#make deal size in millions or billions\n",
    "# final['deal_size_text'] = final.deal_size.apply(milli_billi, 1)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'deal_size'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-6cd0919e14e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#make deal size in millions or billions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfinal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'deal_size_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeal_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmilli_billi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/vitalii/.local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'deal_size'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "final.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>deal_text</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>story_url</th>\n",
       "      <th>company_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ Lordstown Motors Corp., the electric-truck m...</td>\n",
       "      <td>[September 30, 2021]</td>\n",
       "      <td>/articles/lordstown-nears-deal-sell-ohio-plant...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ ABB Ltd. is preparing to roll out the first ...</td>\n",
       "      <td>[September 30, 2021]</td>\n",
       "      <td>/articles/new-ev-charger-can-provide-62-miles-...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ After sounding an optimistic note in the fir...</td>\n",
       "      <td>[September 30, 2021]</td>\n",
       "      <td>/articles/car-sales-drop-chip-shortages-stymie...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ WASHINGTON â€”Â His government overhaul plans a...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/progress-budget-talks-will-deal-beat...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ The outlook for transportation and logistics...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/what-q4-and-beyond-have-store-transp...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  company                                          deal_text  \\\n",
       "0  TTNews  [ Lordstown Motors Corp., the electric-truck m...   \n",
       "1  TTNews  [ ABB Ltd. is preparing to roll out the first ...   \n",
       "2  TTNews  [ After sounding an optimistic note in the fir...   \n",
       "3  TTNews  [ WASHINGTON â€”Â His government overhaul plans a...   \n",
       "4  TTNews  [ The outlook for transportation and logistics...   \n",
       "\n",
       "           publish_date                                          story_url  \\\n",
       "0  [September 30, 2021]  /articles/lordstown-nears-deal-sell-ohio-plant...   \n",
       "1  [September 30, 2021]  /articles/new-ev-charger-can-provide-62-miles-...   \n",
       "2  [September 30, 2021]  /articles/car-sales-drop-chip-shortages-stymie...   \n",
       "3  [September 29, 2021]  /articles/progress-budget-talks-will-deal-beat...   \n",
       "4  [September 29, 2021]  /articles/what-q4-and-beyond-have-store-transp...   \n",
       "\n",
       "           company_url  \n",
       "0  https://ttnews.com/  \n",
       "1  https://ttnews.com/  \n",
       "2  https://ttnews.com/  \n",
       "3  https://ttnews.com/  \n",
       "4  https://ttnews.com/  "
      ]
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "len(final)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "#check previous posts, check to see there are no duplicates\n",
    "# old = pd.read_csv('historical_deals.csv')\n",
    "# dupes = pd.merge(final, old[['company', 'deal_size']], how = 'inner', on = ['company', 'deal_size'])\n",
    "# final = final[~final.company.isin(dupes.company)]"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-f67e64647236>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#check previous posts, check to see there are no duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'historical_deals.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdupes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'company'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deal_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'inner'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'company'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deal_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompany\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdupes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompany\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vitalii/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vitalii/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vitalii/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vitalii/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vitalii/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "len(final)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "final = final[final.company != 'Bolt']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "final"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>deal_text</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>story_url</th>\n",
       "      <th>company_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ Lordstown Motors Corp., the electric-truck m...</td>\n",
       "      <td>[September 30, 2021]</td>\n",
       "      <td>/articles/lordstown-nears-deal-sell-ohio-plant...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ ABB Ltd. is preparing to roll out the first ...</td>\n",
       "      <td>[September 30, 2021]</td>\n",
       "      <td>/articles/new-ev-charger-can-provide-62-miles-...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ After sounding an optimistic note in the fir...</td>\n",
       "      <td>[September 30, 2021]</td>\n",
       "      <td>/articles/car-sales-drop-chip-shortages-stymie...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ WASHINGTON â€”Â His government overhaul plans a...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/progress-budget-talks-will-deal-beat...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ The outlook for transportation and logistics...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/what-q4-and-beyond-have-store-transp...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ LONDON â€” The British governmentâ€™s reserve ta...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/soldiers-will-haul-fuel-ease-uk-crisis</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ Navistar Inc. announced the launch of the re...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/navistar-launches-diesel-version-lat...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ Legal experts and researchers say juries gen...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/negative-jury-attitudes-can-contribu...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ Walmart Inc. is expanding its hiring push wi...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/walmart-stokes-hiring-rush-plan-add-...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ When the first snub-nosed, electric van roll...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/electric-vans-roll-line-once-made-ga...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ Loadsmart launched a platform in partnership...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/loadsmart-home-depot-launch-supply-l...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ As automakers scramble to make electric vehi...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/silicon-valley-answer-ev-question-ca...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ Online shoppers will be buying less this hol...</td>\n",
       "      <td>[September 29, 2021]</td>\n",
       "      <td>/articles/inflation-seen-pushing-online-holida...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ WASHINGTON â€” Pressure mounting, President Jo...</td>\n",
       "      <td>[September 28, 2021]</td>\n",
       "      <td>/articles/big-pressure-biden-dems-trim-35t-fed...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ Lucid Group Inc. has started production on i...</td>\n",
       "      <td>[September 28, 2021]</td>\n",
       "      <td>/articles/lucid-deliver-520-mile-electric-seda...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ WASHINGTON â€” Treasury Secretary Janet Yellen...</td>\n",
       "      <td>[September 28, 2021]</td>\n",
       "      <td>/articles/yellen-warns-delay-raising-debt-limi...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ General Motors Co.â€™s electric van unit plans...</td>\n",
       "      <td>[September 28, 2021]</td>\n",
       "      <td>/articles/gm-debuts-midsize-electric-delivery-van</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ LONDON â€” Long lines are snaking down streets...</td>\n",
       "      <td>[September 28, 2021]</td>\n",
       "      <td>/articles/why-and-how-uk-experiencing-fuel-crisis</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ A highway project vital to the flow of freig...</td>\n",
       "      <td>[September 28, 2021]</td>\n",
       "      <td>/articles/colorados-central-70-project-receive...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>[ Commerce Secretary Gina Raimondo made a fres...</td>\n",
       "      <td>[September 28, 2021]</td>\n",
       "      <td>/articles/raimondo-says-ceos-more-reasonable-t...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>/articles/loadsmart-home-depot-launch-supply-l...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>/articles/big-pressure-biden-dems-trim-35t-fed...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>/articles/missouri-lawmakers-pass-fuel-tax-hik...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>/articles/usps-mail-small-package-delivery-may...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TTNews</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>/articles/lucid-deliver-520-mile-electric-seda...</td>\n",
       "      <td>https://ttnews.com/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   company                                          deal_text  \\\n",
       "0   TTNews  [ Lordstown Motors Corp., the electric-truck m...   \n",
       "1   TTNews  [ ABB Ltd. is preparing to roll out the first ...   \n",
       "2   TTNews  [ After sounding an optimistic note in the fir...   \n",
       "3   TTNews  [ WASHINGTON â€”Â His government overhaul plans a...   \n",
       "4   TTNews  [ The outlook for transportation and logistics...   \n",
       "5   TTNews  [ LONDON â€” The British governmentâ€™s reserve ta...   \n",
       "6   TTNews  [ Navistar Inc. announced the launch of the re...   \n",
       "7   TTNews  [ Legal experts and researchers say juries gen...   \n",
       "8   TTNews  [ Walmart Inc. is expanding its hiring push wi...   \n",
       "9   TTNews  [ When the first snub-nosed, electric van roll...   \n",
       "10  TTNews  [ Loadsmart launched a platform in partnership...   \n",
       "11  TTNews  [ As automakers scramble to make electric vehi...   \n",
       "12  TTNews  [ Online shoppers will be buying less this hol...   \n",
       "13  TTNews  [ WASHINGTON â€” Pressure mounting, President Jo...   \n",
       "14  TTNews  [ Lucid Group Inc. has started production on i...   \n",
       "15  TTNews  [ WASHINGTON â€” Treasury Secretary Janet Yellen...   \n",
       "16  TTNews  [ General Motors Co.â€™s electric van unit plans...   \n",
       "17  TTNews  [ LONDON â€” Long lines are snaking down streets...   \n",
       "18  TTNews  [ A highway project vital to the flow of freig...   \n",
       "19  TTNews  [ Commerce Secretary Gina Raimondo made a fres...   \n",
       "20  TTNews                                               None   \n",
       "21  TTNews                                               None   \n",
       "22  TTNews                                               None   \n",
       "23  TTNews                                               None   \n",
       "24  TTNews                                               None   \n",
       "\n",
       "            publish_date                                          story_url  \\\n",
       "0   [September 30, 2021]  /articles/lordstown-nears-deal-sell-ohio-plant...   \n",
       "1   [September 30, 2021]  /articles/new-ev-charger-can-provide-62-miles-...   \n",
       "2   [September 30, 2021]  /articles/car-sales-drop-chip-shortages-stymie...   \n",
       "3   [September 29, 2021]  /articles/progress-budget-talks-will-deal-beat...   \n",
       "4   [September 29, 2021]  /articles/what-q4-and-beyond-have-store-transp...   \n",
       "5   [September 29, 2021]   /articles/soldiers-will-haul-fuel-ease-uk-crisis   \n",
       "6   [September 29, 2021]  /articles/navistar-launches-diesel-version-lat...   \n",
       "7   [September 29, 2021]  /articles/negative-jury-attitudes-can-contribu...   \n",
       "8   [September 29, 2021]  /articles/walmart-stokes-hiring-rush-plan-add-...   \n",
       "9   [September 29, 2021]  /articles/electric-vans-roll-line-once-made-ga...   \n",
       "10  [September 29, 2021]  /articles/loadsmart-home-depot-launch-supply-l...   \n",
       "11  [September 29, 2021]  /articles/silicon-valley-answer-ev-question-ca...   \n",
       "12  [September 29, 2021]  /articles/inflation-seen-pushing-online-holida...   \n",
       "13  [September 28, 2021]  /articles/big-pressure-biden-dems-trim-35t-fed...   \n",
       "14  [September 28, 2021]  /articles/lucid-deliver-520-mile-electric-seda...   \n",
       "15  [September 28, 2021]  /articles/yellen-warns-delay-raising-debt-limi...   \n",
       "16  [September 28, 2021]  /articles/gm-debuts-midsize-electric-delivery-van   \n",
       "17  [September 28, 2021]  /articles/why-and-how-uk-experiencing-fuel-crisis   \n",
       "18  [September 28, 2021]  /articles/colorados-central-70-project-receive...   \n",
       "19  [September 28, 2021]  /articles/raimondo-says-ceos-more-reasonable-t...   \n",
       "20                  None  /articles/loadsmart-home-depot-launch-supply-l...   \n",
       "21                  None  /articles/big-pressure-biden-dems-trim-35t-fed...   \n",
       "22                  None  /articles/missouri-lawmakers-pass-fuel-tax-hik...   \n",
       "23                  None  /articles/usps-mail-small-package-delivery-may...   \n",
       "24                  None  /articles/lucid-deliver-520-mile-electric-seda...   \n",
       "\n",
       "            company_url  \n",
       "0   https://ttnews.com/  \n",
       "1   https://ttnews.com/  \n",
       "2   https://ttnews.com/  \n",
       "3   https://ttnews.com/  \n",
       "4   https://ttnews.com/  \n",
       "5   https://ttnews.com/  \n",
       "6   https://ttnews.com/  \n",
       "7   https://ttnews.com/  \n",
       "8   https://ttnews.com/  \n",
       "9   https://ttnews.com/  \n",
       "10  https://ttnews.com/  \n",
       "11  https://ttnews.com/  \n",
       "12  https://ttnews.com/  \n",
       "13  https://ttnews.com/  \n",
       "14  https://ttnews.com/  \n",
       "15  https://ttnews.com/  \n",
       "16  https://ttnews.com/  \n",
       "17  https://ttnews.com/  \n",
       "18  https://ttnews.com/  \n",
       "19  https://ttnews.com/  \n",
       "20  https://ttnews.com/  \n",
       "21  https://ttnews.com/  \n",
       "22  https://ttnews.com/  \n",
       "23  https://ttnews.com/  \n",
       "24  https://ttnews.com/  "
      ]
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "#save deal records\n",
    "# new_old = pd.concat([final, old], axis = 0)\n",
    "new_old = pd.concat([final], axis = 0)\n",
    "\n",
    "new_old.to_csv('historical_deals.csv', index = False)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'old' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-90fb427fd6ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#save deal records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnew_old\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'historical_deals.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'old' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(new_old)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2019"
      ]
     },
     "metadata": {},
     "execution_count": 304
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "date = 'Sunday, 08/29/2021'\n",
    "\n",
    "title = '''\n",
    "<h1>\n",
    "  Funded & Hiring\n",
    "</h1>\n",
    "<h2 style=\"text-align:center\">\n",
    "  <strong>{date}</strong>\n",
    "</h2>\n",
    "<h2>\n",
    "  A weekly roundup of funded startups and the jobs they're hiring for.\n",
    "</h2>\n",
    "<h4>\n",
    "  Forwarded from a friend? \n",
    "  <a href = 'https://fundedandhiring.com/'>Subscribe</a> to stay up to date on the latest startup funding and job alerts.\n",
    "</h4>\n",
    "'''.format(date = date).strip()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(title)\n",
    "pyperclip.copy(title)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<h1>\n",
      "  Funded & Hiring\n",
      "</h1>\n",
      "<h2 style=\"text-align:center\">\n",
      "  <strong>Sunday, 08/29/2021</strong>\n",
      "</h2>\n",
      "<h2>\n",
      "  A weekly roundup of funded startups and the jobs they're hiring for.\n",
      "</h2>\n",
      "<h4>\n",
      "  Forwarded from a friend? \n",
      "  <a href = 'https://fundedandhiring.com/'>Subscribe</a> to stay up to date on the latest startup funding and job alerts.\n",
      "</h4>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "content = ''''''\n",
    "\n",
    "final_dict = final.to_dict(orient = 'records')\n",
    "for row in final_dict:\n",
    "    headline = \"HEADLINE\"\n",
    "    company_name = row['company']\n",
    "    story_url = row['story_url']\n",
    "    description = row['deal_text']\n",
    "    company_url = row['company_url']\n",
    "    \n",
    "    #get job types and job count\n",
    "    # job_types = [key for key in row.keys() if key not in ['company', 'deal_text', 'publish_date', 'story_url', 'deal_size', 'job_page', 'deal_size_text', 'company_url']]\n",
    "    # job_types_dict = {}\n",
    "    # for typ in job_types:\n",
    "    #     if row[typ] == 0:\n",
    "    #         pass\n",
    "    #     else:\n",
    "    #         job_types_dict[typ] = row[typ]\n",
    "            \n",
    "    # job_types_dict = dict(sorted(job_types_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    #order job types by number of jobs, move Other to the end\n",
    "    # job_type_order = list(job_types_dict.keys())\n",
    "    # if 'Other' in job_type_order:\n",
    "    #     job_type_order.append('Other')\n",
    "    #     del job_type_order[job_type_order.index('Other')]\n",
    "    \n",
    "    hiring_for = ''''''\n",
    "    # for key in job_type_order:\n",
    "    #     job_string = key + '({count})'.format(count = job_types_dict[key]) + ', '\n",
    "    #     hiring_for += job_string\n",
    "    # hiring_for = re.sub(r', $', '', hiring_for)\n",
    "            \n",
    "    \n",
    "    entry = '''\n",
    "    <tr>\n",
    "      <td>\n",
    "        <h3>\n",
    "          <a href = \"{story_url}\">{headline}</a>\n",
    "        </h3>\n",
    "        <h4>\n",
    "          <a href = \"{company_url}\">{company_name}</a>\n",
    "        <strong>Funding Amount</strong>: <em><strong><a href = \"{story_url}\">{deal_size_text}</a></strong></em>\n",
    "        <p>\n",
    "          {description}\n",
    "        </p>\n",
    "        <p>\n",
    "          <strong>Hiring For:</strong>\n",
    "            <a href = \"www.google.com\">\n",
    "              {hiring_for}\n",
    "            </a>\n",
    "        </p>\n",
    "      </td>\n",
    "    </tr> '''.format(story_url = story_url, company_name = company_name, deal_size_text = deal_size_text,deal_text = deal_text, job_page = 'www.google.com', hiring_for = hiring_for, company_url = company_url).strip()\n",
    "\n",
    "    content += '\\n' + entry.strip()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pyperclip.copy(content)\n",
    "print(content)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "<tr>\n",
      "      <td>\n",
      "        <h3>\n",
      "          <a href = \"http://www.graymatter-robotics.com/\">GrayMatter Robotics</a>\n",
      "        </h3>\n",
      "        <strong>Funding Amount</strong>: <em><strong><a href = \"http://www.graymatter-robotics.com/\">$4.1 Million</a></strong></em>\n",
      "        <p>\n",
      "          GrayMatter Robotics, an LA-based maker of manufacturing robots, raised 4.1 million in seed funding. Stage Venture Partners and Calibrate Ventures co-led, and were joined by 3M Ventures, OCA Ventures, Pathbreaker Ventures and B Capital Group.\n",
      "        </p>\n",
      "        <p>\n",
      "          <strong>Hiring For:</strong>\n",
      "            <a href = \"www.google.com\">\n",
      "              \n",
      "            </a>\n",
      "        </p>\n",
      "      </td>\n",
      "    </tr>\n",
      "<tr>\n",
      "      <td>\n",
      "        <h3>\n",
      "          <a href = \"https://www.runx.dev/\">RunX</a>\n",
      "        </h3>\n",
      "        <strong>Funding Amount</strong>: <em><strong><a href = \"http://axios.link/DRIR\">$4.1 Million</a></strong></em>\n",
      "        <p>\n",
      "          RunX, an SF-based cloud infrastructure deployment startup, raised 4.1 million in seed funding led by Unusual Ventures.\n",
      "        </p>\n",
      "        <p>\n",
      "          <strong>Hiring For:</strong>\n",
      "            <a href = \"www.google.com\">\n",
      "              \n",
      "            </a>\n",
      "        </p>\n",
      "      </td>\n",
      "    </tr>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#job boards\n",
    "# stackoverflow, jobvite, bamboohr"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}